{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XN_SAMPLE Alignment Between analysers\n",
    "\n",
    "This notebook demonstrates how to align tabular XN_SAMPLE data between different Sysmex analysers using the `XNSampleTransformer` class.\n",
    "\n",
    "**What is XN_SAMPLE alignment?**\n",
    "- XN_SAMPLE.csv files contain complete blood count (CBC) parameters from Sysmex analysers\n",
    "- Different analysers can produce systematically different measurements for the same sample\n",
    "- MAD (Median Absolute Deviation) / median-based transformation aligns distributions\n",
    "\n",
    "**When to use XN_SAMPLE alignment:**\n",
    "- Combining data from multiple Sysmex analysers, and not wanting to use the GAM-based adjustment (\"correction\" module)\n",
    "- Training ML models on multi-site data\n",
    "- Longitudinal studies where analysers were upgraded/replaced\n",
    "\n",
    "**Method:**\n",
    "- Uses robust statistics (median and MAD) to align distributions, hence it is slightly more sophisticated than just adding a factor term in the GAM method (\"correction\" module)\n",
    "- More robust to outliers than mean/standard deviation\n",
    "- Transforms: `X_transformed = target_median + (X - source_median) * (target_MAD / source_MAD)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "# PDF-compatible fonts\n",
    "matplotlib.rcParams[\"pdf.fonttype\"] = 42\n",
    "matplotlib.rcParams[\"ps.fonttype\"] = 42\n",
    "\n",
    "# Scientific plot style\n",
    "import scienceplots\n",
    "\n",
    "plt.style.use([\"science\", \"nature\"])\n",
    "\n",
    "# Colourblind-friendly palette\n",
    "SEABORN_PALETTE = \"colorblind\"\n",
    "seaborn_colors = sns.color_palette(SEABORN_PALETTE)\n",
    "\n",
    "# Add parent directory to path to import sysmexcbctools\n",
    "# This allows imports to work whether or not the package is installed\n",
    "repo_root = (\n",
    "    Path(__file__).parent.parent.parent\n",
    "    if \"__file__\" in globals()\n",
    "    else Path.cwd().parent.parent\n",
    ")\n",
    "sys.path.insert(0, str(repo_root))\n",
    "\n",
    "from sysmexcbctools.transfer.sysmexalign import XNSampleTransformer\n",
    "from sysmexcbctools.transfer.sysmexalign.alignment_1d import mad\n",
    "from sysmexcbctools.transfer.config import load_config\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "\n",
    "\n",
    "# Function to escape special LaTeX characters in parameter names\n",
    "def latex_escape(text):\n",
    "    \"\"\"Escape special characters for LaTeX rendering.\"\"\"\n",
    "    import re\n",
    "\n",
    "    # FIRST: Escape LaTeX special characters (before adding math mode)\n",
    "    # Save the original parentheses patterns we'll convert to math mode\n",
    "    text = text.replace(\"&\", \"\\\\&\")\n",
    "    text = text.replace(\"%\", \"\\\\%\")\n",
    "    text = text.replace(\"#\", \"\\\\#\")\n",
    "    text = text.replace(\"_\", \"\\\\_\")\n",
    "    text = text.replace(\"~\", \"\\\\textasciitilde\")\n",
    "\n",
    "    # THEN: Handle superscripts in math mode\n",
    "    # Match patterns like (10^3/uL) and convert to LaTeX math mode\n",
    "    # Convert (10^3/uL) -> ($10^{3}$/uL)\n",
    "    text = re.sub(r\"\\((\\d+)\\^(\\d+)/(\\w+)\\)\", r\"($\\1^{\\2}$/\\3)\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load XN_SAMPLE Data from Two analysers\n",
    "\n",
    "We'll use XN_SAMPLE data from two different blood donor studies:\n",
    "- **Source**: STRIDES\n",
    "- **Target**: INTERVAL (analyser 36)\n",
    "\n",
    "These are real data from the INTERVAL and STRIDES study, collected on different Sysmex XN analysers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration to get data paths\n",
    "config = load_config(str(repo_root / \"sysmexcbctools/transfer/config/data_paths.yaml\"))\n",
    "\n",
    "source_path = config[\"datasets\"][\"raw\"][\"strides\"] + \"/\"\n",
    "target_path = config[\"datasets\"][\"raw\"][\"interval_36\"] + \"/XN_SAMPLE.csv\"\n",
    "\n",
    "# Load data\n",
    "print(\"Loading XN_SAMPLE data...\")\n",
    "# source path is actually a folder with multiple subfolders, each containing a XN_SAMPLE.csv\n",
    "import glob\n",
    "\n",
    "source_df = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(f, low_memory=False)\n",
    "        for f in glob.glob(source_path + \"**/XN_SAMPLE.csv\", recursive=True)\n",
    "    ]\n",
    ")\n",
    "target_df = pd.read_csv(target_path, low_memory=False)\n",
    "\n",
    "print(f\"Source data shape: {source_df.shape}\")\n",
    "print(f\"Target data shape: {target_df.shape}\")\n",
    "print(f\"\\nSource columns: {len(source_df.columns)} columns\")\n",
    "print(f\"Target columns: {len(target_df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Columns to Transform\n",
    "\n",
    "We'll transform standard full blood count (FBC) parameters. These are the core clinical measurements that should be aligned between analysers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard FBC columns to transform\n",
    "fbc_columns = [\n",
    "    # Red blood cell parameters\n",
    "    \"RBC(10^6/uL)\",  # Red blood cell count\n",
    "    \"HGB(g/dL)\",  # Hemoglobin\n",
    "    \"HCT(%)\",  # Hematocrit\n",
    "    \"MCV(fL)\",  # Mean corpuscular volume\n",
    "    \"MCH(pg)\",  # Mean corpuscular hemoglobin\n",
    "    \"MCHC(g/dL)\",  # Mean corpuscular hemoglobin concentration\n",
    "    \"RDW-SD(fL)\",  # Red cell distribution width (SD)\n",
    "    \"RDW-CV(%)\",  # Red cell distribution width (CV)\n",
    "    # White blood cell parameters\n",
    "    \"WBC(10^3/uL)\",  # White blood cell count\n",
    "    \"NEUT#(10^3/uL)\",  # Neutrophil count\n",
    "    \"LYMPH#(10^3/uL)\",  # Lymphocyte count\n",
    "    \"MONO#(10^3/uL)\",  # Monocyte count\n",
    "    \"EO#(10^3/uL)\",  # Eosinophil count\n",
    "    \"BASO#(10^3/uL)\",  # Basophil count\n",
    "    \"NEUT%(%)\",  # Neutrophil percentage\n",
    "    \"LYMPH%(%)\",  # Lymphocyte percentage\n",
    "    \"MONO%(%)\",  # Monocyte percentage\n",
    "    \"EO%(%)\",  # Eosinophil percentage\n",
    "    \"BASO%(%)\",  # Basophil percentage\n",
    "    # Platelet parameters\n",
    "    \"PLT(10^3/uL)\",  # Platelet count\n",
    "    \"MPV(fL)\",  # Mean platelet volume\n",
    "    \"PCT(%)\",  # Plateletcrit\n",
    "    \"PDW(fL)\",  # Platelet distribution width\n",
    "]\n",
    "\n",
    "# Check which columns are available in both datasets\n",
    "source_cols_available = [col for col in fbc_columns if col in source_df.columns]\n",
    "target_cols_available = [col for col in fbc_columns if col in target_df.columns]\n",
    "columns_to_transform = list(set(source_cols_available) & set(target_cols_available))\n",
    "\n",
    "print(f\"Requested columns: {len(fbc_columns)}\")\n",
    "print(f\"Available in source: {len(source_cols_available)}\")\n",
    "print(f\"Available in target: {len(target_cols_available)}\")\n",
    "print(f\"Available in both: {len(columns_to_transform)}\")\n",
    "print(f\"\\nColumns to transform: {sorted(columns_to_transform)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Centile Sample Numbers (Optional)\n",
    "\n",
    "We can load sample numbers to use in lieu of an official standard (such as a parallel callibration measurement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline sample numbers\n",
    "# Also load sample numbers for filtering (optional but recommended)\n",
    "source_samples_path = config[\"files\"][\"centile_samples\"][\"strides\"]\n",
    "target_samples_path = config[\"files\"][\"centile_samples\"][\"interval_baseline_36\"]\n",
    "\n",
    "source_sample_nos = np.load(source_samples_path, allow_pickle=True)\n",
    "target_sample_nos = np.load(target_samples_path, allow_pickle=True)\n",
    "\n",
    "# Convert to list of strings for matching\n",
    "source_sample_nos = [str(s).strip() for s in source_sample_nos]\n",
    "target_sample_nos = [str(s).strip() for s in target_sample_nos]\n",
    "\n",
    "print(f\"Source centile samples: {len(source_sample_nos)}\")\n",
    "print(f\"Target centile samples: {len(target_sample_nos)}\")\n",
    "\n",
    "# Check how many samples match\n",
    "source_df[\"Sample No.\"] = source_df[\"Sample No.\"].astype(str).str.strip()\n",
    "target_df[\"Sample No.\"] = target_df[\"Sample No.\"].astype(str).str.strip()\n",
    "\n",
    "source_matched = source_df[\"Sample No.\"].isin(source_sample_nos).sum()\n",
    "target_matched = target_df[\"Sample No.\"].isin(target_sample_nos).sum()\n",
    "\n",
    "print(\n",
    "    f\"\\nMatched samples in source: {source_matched:,} / {len(source_df):,} ({100*source_matched/len(source_df):.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"Matched samples in target: {target_matched:,} / {len(target_df):,} ({100*target_matched/len(target_df):.1f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create and Fit XNSampleTransformer\n",
    "\n",
    "Now we'll create the transformer and fit it using the centile samples. The transformer will compute median and MAD for each column in both source and target distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformer\n",
    "transformer = XNSampleTransformer(columns=columns_to_transform)\n",
    "\n",
    "# Fit transformer\n",
    "print(\"Fitting XNSampleTransformer...\\n\")\n",
    "transformer.fit(\n",
    "    source_df=source_df,\n",
    "    target_df=target_df,\n",
    "    source_sample_nos=source_sample_nos,\n",
    "    target_sample_nos=target_sample_nos,\n",
    ")\n",
    "\n",
    "print(\"\\nTransformer fitted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transform Source Data\n",
    "\n",
    "Apply the fitted transformation to align source distribution to target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the source data\n",
    "print(\"Transforming source data...\\n\")\n",
    "transformed_df = transformer.transform(source_df.copy())\n",
    "\n",
    "print(f\"\\nTransformed data shape: {transformed_df.shape}\")\n",
    "print(\"Transformation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Transformation Quality\n",
    "\n",
    "Let's evaluate how well the transformation aligns the distributions using multiple metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute evaluation metrics for baseline samples\n",
    "def evaluate_transformation(\n",
    "    source_df, transformed_df, target_df, sample_nos_source, sample_nos_target, columns\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate transformation quality using multiple metrics.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for col in columns:\n",
    "        # Filter to baseline samples\n",
    "        source_vals = pd.to_numeric(\n",
    "            source_df.loc[source_df[\"Sample No.\"].isin(sample_nos_source), col],\n",
    "            errors=\"coerce\",\n",
    "        ).dropna()\n",
    "\n",
    "        transformed_vals = pd.to_numeric(\n",
    "            transformed_df.loc[\n",
    "                transformed_df[\"Sample No.\"].isin(sample_nos_source), col\n",
    "            ],\n",
    "            errors=\"coerce\",\n",
    "        ).dropna()\n",
    "\n",
    "        target_vals = pd.to_numeric(\n",
    "            target_df.loc[target_df[\"Sample No.\"].isin(sample_nos_target), col],\n",
    "            errors=\"coerce\",\n",
    "        ).dropna()\n",
    "\n",
    "        if len(source_vals) < 10 or len(target_vals) < 10:\n",
    "            continue\n",
    "\n",
    "        # Compute statistics\n",
    "        source_median = source_vals.median()\n",
    "        source_mad = mad(source_vals)\n",
    "\n",
    "        transformed_median = transformed_vals.median()\n",
    "        transformed_mad = mad(transformed_vals)\n",
    "\n",
    "        target_median = target_vals.median()\n",
    "        target_mad = mad(target_vals)\n",
    "\n",
    "        # Median difference (before and after)\n",
    "        median_diff_before = abs(source_median - target_median)\n",
    "        median_diff_after = abs(transformed_median - target_median)\n",
    "        median_improvement = (\n",
    "            100 * (median_diff_before - median_diff_after) / median_diff_before\n",
    "            if median_diff_before > 0\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        # MAD difference (before and after)\n",
    "        mad_diff_before = abs(source_mad - target_mad)\n",
    "        mad_diff_after = abs(transformed_mad - target_mad)\n",
    "        mad_improvement = (\n",
    "            100 * (mad_diff_before - mad_diff_after) / mad_diff_before\n",
    "            if mad_diff_before > 0\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        # Wasserstein distance (1D)\n",
    "        wasserstein_before = stats.wasserstein_distance(source_vals, target_vals)\n",
    "        wasserstein_after = stats.wasserstein_distance(transformed_vals, target_vals)\n",
    "        wasserstein_improvement = (\n",
    "            100 * (wasserstein_before - wasserstein_after) / wasserstein_before\n",
    "            if wasserstein_before > 0\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        # KS statistic (goodness of fit test)\n",
    "        ks_before = stats.ks_2samp(source_vals, target_vals).statistic\n",
    "        ks_after = stats.ks_2samp(transformed_vals, target_vals).statistic\n",
    "        ks_improvement = (\n",
    "            100 * (ks_before - ks_after) / ks_before if ks_before > 0 else 0\n",
    "        )\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"Column\": col,\n",
    "                \"Median (Source)\": source_median,\n",
    "                \"Median (Transformed)\": transformed_median,\n",
    "                \"Median (Target)\": target_median,\n",
    "                \"Median Δ Before\": median_diff_before,\n",
    "                \"Median Δ After\": median_diff_after,\n",
    "                \"Median Improvement %\": median_improvement,\n",
    "                \"MAD (Source)\": source_mad,\n",
    "                \"MAD (Transformed)\": transformed_mad,\n",
    "                \"MAD (Target)\": target_mad,\n",
    "                \"MAD Δ Before\": mad_diff_before,\n",
    "                \"MAD Δ After\": mad_diff_after,\n",
    "                \"MAD Improvement %\": mad_improvement,\n",
    "                \"Wasserstein Before\": wasserstein_before,\n",
    "                \"Wasserstein After\": wasserstein_after,\n",
    "                \"Wasserstein Improvement %\": wasserstein_improvement,\n",
    "                \"KS Before\": ks_before,\n",
    "                \"KS After\": ks_after,\n",
    "                \"KS Improvement %\": ks_improvement,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Evaluate transformation\n",
    "print(\"Evaluating transformation quality...\\n\")\n",
    "eval_df = evaluate_transformation(\n",
    "    source_df,\n",
    "    transformed_df,\n",
    "    target_df,\n",
    "    source_sample_nos,\n",
    "    target_sample_nos,\n",
    "    columns_to_transform,\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"Transformation Quality Metrics:\")\n",
    "print(\"=\" * 80)\n",
    "display(\n",
    "    eval_df[\n",
    "        [\n",
    "            \"Column\",\n",
    "            \"Median Improvement %\",\n",
    "            \"MAD Improvement %\",\n",
    "            \"Wasserstein Improvement %\",\n",
    "            \"KS Improvement %\",\n",
    "        ]\n",
    "    ].round(2)\n",
    ")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(f\"Average Median Improvement: {eval_df['Median Improvement %'].mean():.2f}%\")\n",
    "print(f\"Average MAD Improvement: {eval_df['MAD Improvement %'].mean():.2f}%\")\n",
    "print(\n",
    "    f\"Average Wasserstein Improvement: {eval_df['Wasserstein Improvement %'].mean():.2f}%\"\n",
    ")\n",
    "print(f\"Average KS Improvement: {eval_df['KS Improvement %'].mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Transformation Results\n",
    "\n",
    "Let's visualize the transformation for key FBC parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select key columns to visualize\n",
    "viz_columns = [\n",
    "    \"HGB(g/dL)\",\n",
    "    \"RBC(10^6/uL)\",\n",
    "    \"WBC(10^3/uL)\",\n",
    "    \"PLT(10^3/uL)\",\n",
    "    \"MCV(fL)\",\n",
    "    \"NEUT#(10^3/uL)\",\n",
    "]\n",
    "viz_columns = [col for col in viz_columns if col in columns_to_transform]\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(2, 3, figsize=(6.6, 4.5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, col in enumerate(viz_columns[:6]):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    # Extract data for baseline samples\n",
    "    source_vals = pd.to_numeric(\n",
    "        source_df.loc[source_df[\"Sample No.\"].isin(source_sample_nos), col],\n",
    "        errors=\"coerce\",\n",
    "    ).dropna()\n",
    "\n",
    "    transformed_vals = pd.to_numeric(\n",
    "        transformed_df.loc[transformed_df[\"Sample No.\"].isin(source_sample_nos), col],\n",
    "        errors=\"coerce\",\n",
    "    ).dropna()\n",
    "\n",
    "    target_vals = pd.to_numeric(\n",
    "        target_df.loc[target_df[\"Sample No.\"].isin(target_sample_nos), col],\n",
    "        errors=\"coerce\",\n",
    "    ).dropna()\n",
    "\n",
    "    # Plot histograms\n",
    "    ax.hist(\n",
    "        source_vals, bins=50, alpha=0.5, label=\"Source (36)\", density=True, color=\"blue\"\n",
    "    )\n",
    "    ax.hist(\n",
    "        transformed_vals,\n",
    "        bins=50,\n",
    "        alpha=0.5,\n",
    "        label=\"Transformed\",\n",
    "        density=True,\n",
    "        color=\"green\",\n",
    "    )\n",
    "    ax.hist(\n",
    "        target_vals,\n",
    "        bins=50,\n",
    "        alpha=0.5,\n",
    "        label=\"Target (41)\",\n",
    "        density=True,\n",
    "        color=\"orange\",\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(latex_escape(col))\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.legend()\n",
    "    ax.set_title(latex_escape(col))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    \"../outputs/xnsample_transformation_distributions.png\", dpi=300, bbox_inches=\"tight\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(ax.set_xlabel(latex_escape(col)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Quantile-Quantile Plots\n",
    "\n",
    "Q-Q plots show how well the transformed distribution matches the target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Q-Q plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(6.6, 4.5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, col in enumerate(viz_columns[:6]):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    # Extract data\n",
    "    transformed_vals = (\n",
    "        pd.to_numeric(\n",
    "            transformed_df.loc[\n",
    "                transformed_df[\"Sample No.\"].isin(source_sample_nos), col\n",
    "            ],\n",
    "            errors=\"coerce\",\n",
    "        )\n",
    "        .dropna()\n",
    "        .values\n",
    "    )\n",
    "\n",
    "    target_vals = (\n",
    "        pd.to_numeric(\n",
    "            target_df.loc[target_df[\"Sample No.\"].isin(target_sample_nos), col],\n",
    "            errors=\"coerce\",\n",
    "        )\n",
    "        .dropna()\n",
    "        .values\n",
    "    )\n",
    "\n",
    "    # Compute quantiles\n",
    "    quantiles = np.linspace(0, 100, 100)\n",
    "    transformed_quantiles = np.percentile(transformed_vals, quantiles)\n",
    "    target_quantiles = np.percentile(target_vals, quantiles)\n",
    "\n",
    "    # Plot Q-Q\n",
    "    ax.scatter(target_quantiles, transformed_quantiles, alpha=0.6, s=20)\n",
    "\n",
    "    # Add diagonal line (perfect alignment)\n",
    "    min_val = min(target_quantiles.min(), transformed_quantiles.min())\n",
    "    max_val = max(target_quantiles.max(), transformed_quantiles.max())\n",
    "    ax.plot(\n",
    "        [min_val, max_val],\n",
    "        [min_val, max_val],\n",
    "        \"r--\",\n",
    "        linewidth=2,\n",
    "        label=\"Perfect alignment\",\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(f\"Target (41) Quantiles\")\n",
    "    ax.set_ylabel(f\"Transformed Quantiles\")\n",
    "    ax.set_title(latex_escape(col))\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    \"../outputs/xnsample_transformation_qqplots.png\", dpi=300, bbox_inches=\"tight\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"Q-Q plots saved to ../outputs/xnsample_transformation_qqplots.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save and Load Transformer\n",
    "\n",
    "You can save the fitted transformer and reload it later for transforming new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = \"../outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save transformer\n",
    "transformer_path = f\"{output_dir}/xnsample_transformer_36_to_41.pkl\"\n",
    "transformer.save(transformer_path)\n",
    "\n",
    "# Load transformer\n",
    "print(\"\\nTesting load functionality...\")\n",
    "loaded_transformer = XNSampleTransformer.load(transformer_path)\n",
    "\n",
    "# Verify loaded transformer works\n",
    "test_df = source_df.head(100).copy()\n",
    "test_transformed = loaded_transformer.transform(test_df)\n",
    "print(f\"\\nTest transformation successful! Transformed {len(test_transformed)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Compare Correlations\n",
    "\n",
    "Let's check if the transformation preserves biological correlations between parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of columns for correlation analysis\n",
    "corr_columns = [\n",
    "    \"HGB(g/dL)\",\n",
    "    \"RBC(10^6/uL)\",\n",
    "    \"HCT(%)\",\n",
    "    \"MCV(fL)\",\n",
    "    \"WBC(10^3/uL)\",\n",
    "    \"PLT(10^3/uL)\",\n",
    "]\n",
    "corr_columns = [col for col in corr_columns if col in columns_to_transform]\n",
    "\n",
    "# Compute correlations for baseline samples\n",
    "source_corr_data = (\n",
    "    source_df.loc[source_df[\"Sample No.\"].isin(source_sample_nos), corr_columns]\n",
    "    .apply(pd.to_numeric, errors=\"coerce\")\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "transformed_corr_data = (\n",
    "    transformed_df.loc[\n",
    "        transformed_df[\"Sample No.\"].isin(source_sample_nos), corr_columns\n",
    "    ]\n",
    "    .apply(pd.to_numeric, errors=\"coerce\")\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "target_corr_data = (\n",
    "    target_df.loc[target_df[\"Sample No.\"].isin(target_sample_nos), corr_columns]\n",
    "    .apply(pd.to_numeric, errors=\"coerce\")\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "# Compute correlation matrices\n",
    "source_corr = source_corr_data.corr()\n",
    "transformed_corr = transformed_corr_data.corr()\n",
    "target_corr = target_corr_data.corr()\n",
    "\n",
    "# Apply LaTeX escaping to column and index names before plotting\n",
    "source_corr_display = source_corr.copy()\n",
    "source_corr_display.columns = [latex_escape(col) for col in source_corr_display.columns]\n",
    "source_corr_display.index = [latex_escape(idx) for idx in source_corr_display.index]\n",
    "\n",
    "transformed_corr_display = transformed_corr.copy()\n",
    "transformed_corr_display.columns = [\n",
    "    latex_escape(col) for col in transformed_corr_display.columns\n",
    "]\n",
    "transformed_corr_display.index = [\n",
    "    latex_escape(idx) for idx in transformed_corr_display.index\n",
    "]\n",
    "\n",
    "target_corr_display = target_corr.copy()\n",
    "target_corr_display.columns = [latex_escape(col) for col in target_corr_display.columns]\n",
    "target_corr_display.index = [latex_escape(idx) for idx in target_corr_display.index]\n",
    "\n",
    "# Plot correlation matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(9.9, 3.3))\n",
    "\n",
    "sns.heatmap(\n",
    "    source_corr_display,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    ax=axes[0],\n",
    "    cbar_kws={\"label\": \"Correlation\"},\n",
    ")\n",
    "axes[0].set_title(\"Source (36) Correlations\")\n",
    "\n",
    "sns.heatmap(\n",
    "    transformed_corr_display,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    ax=axes[1],\n",
    "    cbar_kws={\"label\": \"Correlation\"},\n",
    ")\n",
    "axes[1].set_title(\"Transformed Correlations\")\n",
    "\n",
    "sns.heatmap(\n",
    "    target_corr_display,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    ax=axes[2],\n",
    "    cbar_kws={\"label\": \"Correlation\"},\n",
    ")\n",
    "axes[2].set_title(\"Target (41) Correlations\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    \"../outputs/xnsample_transformation_correlations.png\", dpi=300, bbox_inches=\"tight\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Compute correlation difference\n",
    "print(\"\\nCorrelation Structure Preservation:\")\n",
    "print(\n",
    "    f\"Mean absolute difference (Source vs Target): {np.abs(source_corr.values - target_corr.values).mean():.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Mean absolute difference (Transformed vs Target): {np.abs(transformed_corr.values - target_corr.values).mean():.4f}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bloodcounts_sysmex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
