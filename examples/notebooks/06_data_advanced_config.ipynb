{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Configuration for XNSampleProcessor\n",
    "\n",
    "This notebook provides a deep dive into all processing parameters and advanced features of the `XNSampleProcessor` class.\n",
    "\n",
    "## Topics Covered\n",
    "\n",
    "1. All Processing Parameters Explained\n",
    "2. keep_drop_rows Mode - Preview Before Dropping\n",
    "3. std_threshold Tuning\n",
    "4. make_dummy_marks Parameter\n",
    "5. Memory Optimisation Settings\n",
    "6. Correlation Analysis Deep Dive\n",
    "7. Custom Cleaning Workflows\n",
    "8. Performance Tuning\n",
    "9. Integration with Other Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# PDF-compatible fonts\n",
    "matplotlib.rcParams[\"pdf.fonttype\"] = 42\n",
    "matplotlib.rcParams[\"ps.fonttype\"] = 42\n",
    "\n",
    "# Scientific plot style\n",
    "import scienceplots\n",
    "\n",
    "plt.style.use([\"science\", \"nature\"])\n",
    "\n",
    "# Colourblind-friendly palette\n",
    "SEABORN_PALETTE = \"colorblind\"\n",
    "seaborn_colors = sns.color_palette(SEABORN_PALETTE)\n",
    "\n",
    "from sysmexcbctools.data import XNSampleProcessor\n",
    "\n",
    "# For nice display\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. All Processing Parameters Explained\n",
    "\n",
    "Let's review every parameter available in `XNSampleProcessor` and what they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a default processor to see all parameters\n",
    "default_processor = XNSampleProcessor()\n",
    "\n",
    "print(\"XNSampleProcessor Parameters:\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "params = [\n",
    "    (\n",
    "        \"config_path\",\n",
    "        default_processor.config_path,\n",
    "        \"Path to YAML config file (overrides other params)\",\n",
    "    ),\n",
    "    (\n",
    "        \"remove_clotintube\",\n",
    "        default_processor.remove_clotintube,\n",
    "        \"Remove samples with clot indicators (turbidity, agglutination, PLT clumps)\",\n",
    "    ),\n",
    "    (\n",
    "        \"remove_multimeasurementsamples\",\n",
    "        default_processor.remove_multimeasurementsamples,\n",
    "        \"Handle multiple measurements per sample\",\n",
    "    ),\n",
    "    (\n",
    "        \"remove_correlated\",\n",
    "        default_processor.remove_correlated,\n",
    "        \"Remove highly correlated columns (NOT RECOMMENDED)\",\n",
    "    ),\n",
    "    (\n",
    "        \"std_threshold\",\n",
    "        default_processor.std_threshold,\n",
    "        \"SD threshold for comparing multiple measurements\",\n",
    "    ),\n",
    "    (\n",
    "        \"keep_drop_rows\",\n",
    "        default_processor.keep_drop_rows,\n",
    "        \"Keep rows but mark for dropping (preview mode)\",\n",
    "    ),\n",
    "    (\n",
    "        \"make_dummy_marks\",\n",
    "        default_processor.make_dummy_marks,\n",
    "        \"One-hot encode data mark fields (/M columns)\",\n",
    "    ),\n",
    "    (\n",
    "        \"use_memory_optimized\",\n",
    "        default_processor.use_memory_optimized,\n",
    "        \"Use memory-efficient processing for large datasets\",\n",
    "    ),\n",
    "    (\n",
    "        \"enable_memory_monitoring\",\n",
    "        default_processor.enable_memory_monitoring,\n",
    "        \"Log memory usage throughout processing\",\n",
    "    ),\n",
    "    (\n",
    "        \"correlation_sample_size\",\n",
    "        default_processor.correlation_sample_size,\n",
    "        \"Max rows for correlation analysis\",\n",
    "    ),\n",
    "    (\n",
    "        \"chunk_size\",\n",
    "        default_processor.chunk_size,\n",
    "        \"Chunk size for processing multiple measurements\",\n",
    "    ),\n",
    "    (\"force_dask\", default_processor.force_dask, \"Force Dask usage (for testing)\"),\n",
    "    (\"output_dir\", default_processor.output_dir, \"Directory for output files\"),\n",
    "    (\"output_prefix\", default_processor.output_prefix, \"Prefix for output filenames\"),\n",
    "    (\"log_to_file\", default_processor.log_to_file, \"Save logs and diagnostic files\"),\n",
    "    (\n",
    "        \"verbose\",\n",
    "        default_processor.verbose,\n",
    "        \"Verbosity level (0=silent, 1=info, 2=debug)\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "for param_name, default_value, description in params:\n",
    "    print(f\"{param_name:30s} = {str(default_value):15s}  # {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. keep_drop_rows Mode - Preview Before Dropping\n",
    "\n",
    "Sometimes you want to see what would be removed without actually removing it. The `keep_drop_rows` parameter adds indicator columns instead of dropping rows (shoutouts to Andrew Gibbs for this feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process with keep_drop_rows=True\n",
    "processor_preview = XNSampleProcessor(keep_drop_rows=True, verbose=1)\n",
    "\n",
    "df_preview = processor_preview.process_files(\n",
    "    \"../data/temp/XN_SAMPLE_part1.csv\", save_output=False\n",
    ")\n",
    "\n",
    "print(f\"\\nDataframe shape: {df_preview.shape}\")\n",
    "print(f\"\\nColumns starting with 'drop_':\")\n",
    "drop_cols = [col for col in df_preview.columns if col.startswith(\"drop_\")]\n",
    "print(drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse what would be dropped\n",
    "if drop_cols:\n",
    "    print(\"Samples marked for dropping:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for col in drop_cols:\n",
    "        n_dropped = int(df_preview[col].sum())\n",
    "        pct = n_dropped / len(df_preview) * 100\n",
    "        print(f\"{col:30s}: {n_dropped:6d} ({pct:5.2f}%)\")\n",
    "\n",
    "    # Overall drop column (union of all drop reasons)\n",
    "    if \"drop\" in df_preview.columns:\n",
    "        total_drop = df_preview[\"drop\"].sum()\n",
    "        pct_total = total_drop / len(df_preview) * 100\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(f\"{'TOTAL (union)':30s}: {total_drop:6d} ({pct_total:5.2f}%)\")\n",
    "\n",
    "        # Samples that would be kept\n",
    "        n_kept = (~df_preview[\"drop\"]).sum()\n",
    "        pct_kept = n_kept / len(df_preview) * 100\n",
    "        print(f\"{'Samples kept':30s}: {n_kept:6d} ({pct_kept:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise drop reasons\n",
    "if drop_cols and len(drop_cols) > 0:\n",
    "    drop_counts = df_preview[drop_cols].sum().sort_values(ascending=True)\n",
    "\n",
    "    plt.figure(figsize=(4.5, 2.2))\n",
    "    drop_counts.plot(kind=\"barh\", color=\"salmon\")\n",
    "    plt.xlabel(\"Number of samples\")\n",
    "    plt.title(\"Samples Marked for Dropping by Reason\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Inspect samples that would be dropped due to PLT clumping\n",
    "if \"drop_IP SUS(PLT)PLT Clumps?\" in df_preview.columns:\n",
    "    clumped_samples = df_preview[df_preview[\"drop_IP SUS(PLT)PLT Clumps?\"] == True]\n",
    "\n",
    "    print(f\"\\nFound {len(clumped_samples)} samples with PLT clumping indicators\")\n",
    "\n",
    "    if len(clumped_samples) > 0:\n",
    "        # Show a few examples\n",
    "        print(\"\\nExample clumped samples:\")\n",
    "        display_cols = [\"Sample No.\", \"Date\", \"WBC(10^3/uL)\", \"PLT(10^3/uL)\"]\n",
    "        display_cols = [c for c in display_cols if c in clumped_samples.columns]\n",
    "        print(clumped_samples[display_cols].head())\n",
    "\n",
    "    print(\"\\nClumping samples platelet volume distribution difference:\")\n",
    "    sns.displot(\n",
    "        data=df_preview,\n",
    "        x=\"MPV(fL)\",\n",
    "        hue=\"drop_IP SUS(PLT)PLT Clumps?\",\n",
    "        kind=\"kde\",\n",
    "        fill=True,\n",
    "        common_norm=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. std_threshold Tuning\n",
    "\n",
    "The `std_threshold` parameter controls how close multiple measurements of the same sample have to be to each other to be considered correct and retained (this may not be necessary at all if you just take the samples from your respective electronic health records system, as that should have been properly checked by the haematology lab staff before being entered). Let's explore its impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different thresholds\n",
    "thresholds = [0.5, 1.0, 1.5, 2.0]\n",
    "results = {}\n",
    "\n",
    "print(\"Processing with different std_threshold values...\\n\")\n",
    "\n",
    "for threshold in thresholds:\n",
    "    processor = XNSampleProcessor(std_threshold=threshold, verbose=0)  # Quiet mode\n",
    "\n",
    "    df = processor.process_files(\"../data/temp/XN_SAMPLE_part1.csv\", save_output=False)\n",
    "\n",
    "    results[threshold] = {\n",
    "        \"n_rows\": len(df),\n",
    "        \"n_samples\": df[\"Sample No.\"].nunique(),\n",
    "        \"dataframe\": df,\n",
    "    }\n",
    "\n",
    "# Compare results\n",
    "print(\"Impact of std_threshold on data retention:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Threshold':>12s} {'Rows':>10s} {'Samples':>10s} {'Change vs 1.0':>15s}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "baseline_rows = results[1.0][\"n_rows\"]\n",
    "\n",
    "for threshold in thresholds:\n",
    "    r = results[threshold]\n",
    "    change = r[\"n_rows\"] - baseline_rows\n",
    "    change_pct = (change / baseline_rows) * 100 if baseline_rows > 0 else 0\n",
    "    print(\n",
    "        f\"{threshold:12.1f} {r['n_rows']:10d} {r['n_samples']:10d} {change:+6d} ({change_pct:+5.2f}%)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the impact\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(4.5, 2.2))\n",
    "\n",
    "# Plot 1: Number of rows retained\n",
    "rows_retained = [results[t][\"n_rows\"] for t in thresholds]\n",
    "ax1.plot(thresholds, rows_retained, marker=\"o\", linewidth=2, markersize=8)\n",
    "ax1.set_xlabel(\"std_threshold\")\n",
    "ax1.set_ylabel(\"Number of rows retained\")\n",
    "ax1.set_title(\"Impact of std_threshold on Data Retention\")\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axvline(x=1.0, color=\"red\", linestyle=\"--\", alpha=0.5, label=\"Default (1.0)\")\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Percentage change vs default\n",
    "pct_changes = [\n",
    "    (results[t][\"n_rows\"] - baseline_rows) / baseline_rows * 100 for t in thresholds\n",
    "]\n",
    "ax2.bar(\n",
    "    thresholds,\n",
    "    pct_changes,\n",
    "    width=0.3,\n",
    "    alpha=0.7,\n",
    "    color=[\"salmon\" if x < 0 else \"lightblue\" for x in pct_changes],\n",
    ")\n",
    "ax2.set_xlabel(\"std_threshold\")\n",
    "ax2.set_ylabel(\"% Change vs default (1.0)\")\n",
    "ax2.set_title(\"Relative Change in Data Retention\")\n",
    "ax2.axhline(y=0, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "ax2.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. make_dummy_marks Parameter\n",
    "\n",
    "Data mark columns (ending in `/M`) contain quality indicators. By default they're kept as strings, but you can one-hot encode them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process WITHOUT dummy encoding (default)\n",
    "processor_no_dummy = XNSampleProcessor(make_dummy_marks=False, verbose=0)\n",
    "\n",
    "df_no_dummy = processor_no_dummy.process_files(\n",
    "    \"../data/temp/XN_SAMPLE_part1.csv\", save_output=False\n",
    ")\n",
    "\n",
    "# Find /M columns\n",
    "mark_cols_no_dummy = [col for col in df_no_dummy.columns if col.endswith(\"/M\")]\n",
    "\n",
    "print(\"WITHOUT make_dummy_marks:\")\n",
    "print(f\"  Shape: {df_no_dummy.shape}\")\n",
    "print(f\"  Columns ending in /M: {len(mark_cols_no_dummy)}\")\n",
    "print(f\"\\nExample /M column values:\")\n",
    "if mark_cols_no_dummy:\n",
    "    example_col = mark_cols_no_dummy[0]\n",
    "    print(f\"  {example_col}: {df_no_dummy[example_col].value_counts().head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process WITH dummy encoding\n",
    "processor_dummy = XNSampleProcessor(make_dummy_marks=True, verbose=0)\n",
    "\n",
    "df_dummy = processor_dummy.process_files(\n",
    "    \"../data/temp/XN_SAMPLE_part1.csv\", save_output=False\n",
    ")\n",
    "\n",
    "print(\"\\nWITH make_dummy_marks:\")\n",
    "print(f\"  Shape: {df_dummy.shape}\")\n",
    "print(f\"  Additional columns created: {df_dummy.shape[1] - df_no_dummy.shape[1]}\")\n",
    "\n",
    "# Find dummy columns\n",
    "if mark_cols_no_dummy:\n",
    "    base_name = mark_cols_no_dummy[0]\n",
    "    dummy_cols = [\n",
    "        col\n",
    "        for col in df_dummy.columns\n",
    "        if col.startswith(base_name) and col != mark_cols_no_dummy[0]\n",
    "    ]\n",
    "    if dummy_cols:\n",
    "        print(f\"\\nExample: {mark_cols_no_dummy[0]} was expanded to:\")\n",
    "        for col in dummy_cols[:5]:  # Show first 5\n",
    "            print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use make_dummy_marks?\n",
    "\n",
    "- **TRUE**: When feeding data to ML models that need numeric features\n",
    "- **FALSE** (default): When you want to preserve the original mark information, or when you'll filter/analyze marks manually\n",
    "\n",
    "**Note**: One-hot encoding can significantly increase the number of columns if there are many unique mark values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Memory Optimization Settings\n",
    "\n",
    "For large datasets, memory-optimized processing can be crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Test standard vs memory-optimized processing\n",
    "test_file = \"../data/temp/XN_SAMPLE_part1.csv\"\n",
    "\n",
    "print(\"Comparing standard vs memory-optimized processing...\\n\")\n",
    "\n",
    "# Standard processing\n",
    "start = time.time()\n",
    "processor_standard = XNSampleProcessor(\n",
    "    use_memory_optimized=False, enable_memory_monitoring=True, verbose=1\n",
    ")\n",
    "df_standard = processor_standard.process_files(test_file, save_output=False)\n",
    "time_standard = time.time() - start\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Standard processing complete\")\n",
    "print(f\"Time: {time_standard:.2f}s\")\n",
    "print(f\"Result shape: {df_standard.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory-optimized processing\n",
    "start = time.time()\n",
    "processor_optimized = XNSampleProcessor(\n",
    "    use_memory_optimized=True, enable_memory_monitoring=True, chunk_size=1000, verbose=1\n",
    ")\n",
    "df_optimized = processor_optimized.process_files(test_file, save_output=False)\n",
    "time_optimized = time.time() - start\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Memory-optimized processing complete\")\n",
    "print(f\"Time: {time_optimized:.2f}s\")\n",
    "print(f\"Result shape: {df_optimized.shape}\")\n",
    "print(\n",
    "    f\"\\nSpeedup: {time_standard/time_optimized:.2f}x\"\n",
    "    if time_optimized < time_standard\n",
    "    else f\"\\nSlowdown: {time_optimized/time_standard:.2f}x\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Optimization Parameters:\n",
    "\n",
    "- **use_memory_optimized** (default=True): Automatically uses chunked processing for large datasets\n",
    "- **chunk_size** (default=1000): Number of sample IDs to process at once. Lower = less memory, but more overhead\n",
    "- **correlation_sample_size** (default=50000): Subsample for correlation analysis to save memory\n",
    "- **force_dask** (default=False): Force Dask usage for distributed processing (experimental)\n",
    "\n",
    "**When to adjust**: For datasets >100k rows, consider decreasing chunk_size to 500. For datasets >1M rows, try force_dask=True."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Correlation Analysis Deep Dive\n",
    "\n",
    "The processor analyzes correlations between all columns and standard FBC parameters. Let's understand this better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process with correlation analysis saved\n",
    "processor = XNSampleProcessor(\n",
    "    remove_correlated=False,  # Don't remove, just analyze\n",
    "    log_to_file=True,  # Save correlation analysis to file\n",
    "    output_dir=\"./advanced_output\",\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "df = processor.process_files(\n",
    "    \"../data/temp/XN_SAMPLE_part1.csv\",\n",
    "    dataset_name=\"correlation_demo\",\n",
    "    save_output=False,\n",
    ")\n",
    "\n",
    "# Check if correlation file was created\n",
    "correlation_file = Path(\"./advanced_output/correlation_demo_correlated_columns.csv\")\n",
    "if correlation_file.exists():\n",
    "    print(f\"\\n✓ Correlation analysis saved to: {correlation_file}\")\n",
    "    df_corr = pd.read_csv(correlation_file)\n",
    "    print(f\"\\nShape: {df_corr.shape}\")\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(df_corr.head(10))\n",
    "else:\n",
    "    print(\"\\nNo correlation file created (log_to_file=False or no correlations found)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise correlation patterns\n",
    "if correlation_file.exists():\n",
    "    df_corr = pd.read_csv(correlation_file)\n",
    "\n",
    "    # Rename columns for easier access\n",
    "    df_corr = df_corr.rename(\n",
    "        columns={\n",
    "            \"Standard FBC feature\": \"FBC_Parameter\",\n",
    "            \"Correlated features\": \"Feature\",\n",
    "            \"Correlation strength\": \"Correlation\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Function to escape special LaTeX characters in parameter names\n",
    "    def latex_escape(text):\n",
    "        \"\"\"Escape special characters for LaTeX rendering.\"\"\"\n",
    "        import re\n",
    "\n",
    "        # FIRST: Handle superscripts in math mode (before escaping ^)\n",
    "        # Match patterns like (10^3/uL) and convert to LaTeX math mode\n",
    "        text = re.sub(r\"\\(10\\^(\\d+)/(\\w+)\\)\", r\"(10$^\\1$/\\2)\", text)\n",
    "\n",
    "        # THEN: Escape LaTeX special characters\n",
    "        # Order matters - do backslash first\n",
    "        text = text.replace(\"\\\\\", \"\\\\textbackslash\")\n",
    "        text = text.replace(\"&\", \"\\\\&\")\n",
    "        text = text.replace(\"%\", \"\\\\%\")\n",
    "        text = text.replace(\"#\", \"\\\\#\")\n",
    "        text = text.replace(\"_\", \"\\\\_\")\n",
    "        text = text.replace(\"{\", \"\\\\{\")\n",
    "        text = text.replace(\"}\", \"\\\\}\")\n",
    "        text = text.replace(\"~\", \"\\\\textasciitilde\")\n",
    "\n",
    "        return text\n",
    "\n",
    "    # Plot correlation distribution\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(4.5, 2.2))\n",
    "\n",
    "    # Histogram of correlation values\n",
    "    ax1.hist(df_corr[\"Correlation\"].abs(), bins=50, edgecolor=\"black\", alpha=0.7)\n",
    "    ax1.axvline(x=0.8, color=\"red\", linestyle=\"--\", label=\"Threshold (0.8)\")\n",
    "    ax1.set_xlabel(\"Absolute Correlation\")\n",
    "    ax1.set_ylabel(\"Frequency\")\n",
    "    ax1.set_title(\"Distribution of Correlations with FBC Parameters\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Highly correlated features\n",
    "    high_corr = df_corr[df_corr[\"Correlation\"].abs() >= 0.8].sort_values(\n",
    "        \"Correlation\", ascending=False\n",
    "    )\n",
    "    if len(high_corr) > 0:\n",
    "        top_n = min(15, len(high_corr))\n",
    "        high_corr_top = high_corr.head(top_n)\n",
    "\n",
    "        # Escape LaTeX special characters in labels\n",
    "        labels = [\n",
    "            latex_escape(f\"{row['Feature']} vs {row['FBC_Parameter']}\")\n",
    "            for _, row in high_corr_top.iterrows()\n",
    "        ]\n",
    "        ax2.barh(range(len(labels)), high_corr_top[\"Correlation\"].values, alpha=0.7)\n",
    "        ax2.set_yticks(range(len(labels)))\n",
    "        ax2.set_yticklabels(labels, fontsize=8)\n",
    "        ax2.set_xlabel(\"Correlation\")\n",
    "        # Use >= instead of ≥ for LaTeX compatibility\n",
    "        ax2.set_title(f\"Top {top_n} Highly Correlated Features ($|r| \\geq 0.8$)\")\n",
    "        ax2.axvline(x=0.8, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "        ax2.grid(True, alpha=0.3, axis=\"x\")\n",
    "    else:\n",
    "        ax2.text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            r\"No features with $|r| \\geq 0.8$\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            transform=ax2.transAxes,\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nHighly correlated features (|r| >= 0.8): {len(high_corr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Should you use remove_correlated=True?\n",
    "\n",
    "**Generally NO**, for these reasons:\n",
    "\n",
    "1. **Information loss**: Highly correlated features may still contain unique information\n",
    "2. **Context-dependent**: Correlation in one dataset may not hold in another\n",
    "3. **Domain knowledge**: Some \"redundant\" features may be clinically important\n",
    "\n",
    "**When to consider it**:\n",
    "- You have extreme dimensionality issues (thousands of features)\n",
    "- You've manually reviewed the correlation analysis and agree with the removal\n",
    "- You're doing exploratory analysis where interpretability > completeness\n",
    "- You plan to use the features as inputs to an ML model and want to analyse feature importances (gets split among correlated features usually)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Custom Cleaning Workflows\n",
    "\n",
    "You can combine the processor with custom preprocessing steps for specialized workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Add custom filtering before processing\n",
    "\n",
    "# Step 1: Load raw data\n",
    "df_raw = pd.read_csv(\"../data/temp/XN_SAMPLE_part1.csv\", low_memory=False)\n",
    "print(f\"Raw data: {df_raw.shape}\")\n",
    "\n",
    "# Step 2: Apply custom filters\n",
    "# Example: Keep only samples from a specific date range\n",
    "df_raw[\"Date\"] = pd.to_datetime(df_raw[\"Date\"], errors=\"coerce\")\n",
    "date_mask = (df_raw[\"Date\"] >= \"2012-01-01\") & (df_raw[\"Date\"] <= \"2019-12-31\")\n",
    "df_filtered = df_raw[date_mask].copy()\n",
    "print(f\"After date filter: {df_filtered.shape}\")\n",
    "\n",
    "# Step 3: Save temporarily\n",
    "temp_file = \"../data/temp_custom_filtered.csv\"\n",
    "df_filtered.to_csv(temp_file, index=False)\n",
    "\n",
    "# Step 4: Process with XNSampleProcessor\n",
    "processor = XNSampleProcessor(verbose=0)\n",
    "df_clean = processor.process_files(temp_file, save_output=False)\n",
    "print(f\"After cleaning: {df_clean.shape}\")\n",
    "\n",
    "# Cleanup\n",
    "Path(temp_file).unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Post-processing after cleaning\n",
    "\n",
    "# Clean with processor\n",
    "processor = XNSampleProcessor(verbose=0)\n",
    "df = processor.process_files(\"../data/temp/XN_SAMPLE_part1.csv\", save_output=False)\n",
    "\n",
    "print(f\"After standard cleaning: {df.shape}\")\n",
    "\n",
    "# Custom post-processing: Add derived features\n",
    "if \"WBC(10^3/uL)\" in df.columns and \"LYMPH#(10^3/uL)\" in df.columns:\n",
    "    df[\"LYMPH_RATIO\"] = df[\"LYMPH#(10^3/uL)\"] / df[\"WBC(10^3/uL)\"]\n",
    "    print(\"Added LYMPH_RATIO feature\")\n",
    "\n",
    "if \"RBC(10^6/uL)\" in df.columns and \"HGB(g/dL)\" in df.columns:\n",
    "    df[\"MCH_calculated\"] = df[\"HGB(g/dL)\"] / df[\"RBC(10^6/uL)\"] * 10\n",
    "    print(\"Added MCH_calculated feature\")\n",
    "\n",
    "# Custom filtering: Remove extreme outliers\n",
    "if \"WBC(10^3/uL)\" in df.columns:\n",
    "    wbc_before = len(df)\n",
    "    q1 = df[\"WBC(10^3/uL)\"].quantile(0.001)\n",
    "    q99 = df[\"WBC(10^3/uL)\"].quantile(0.999)\n",
    "    df = df[(df[\"WBC(10^3/uL)\"] >= q1) & (df[\"WBC(10^3/uL)\"] <= q99)]\n",
    "    print(f\"Removed {wbc_before - len(df)} extreme WBC outliers\")\n",
    "\n",
    "print(f\"\\nFinal shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Tuning\n",
    "\n",
    "Tips for processing very large datasets efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance tuning configurations\n",
    "\n",
    "print(\"Recommended configurations for different dataset sizes:\\n\")\n",
    "\n",
    "configs = [\n",
    "    {\n",
    "        \"name\": \"Small (<50k rows)\",\n",
    "        \"params\": {\n",
    "            \"use_memory_optimized\": False,\n",
    "            \"enable_memory_monitoring\": False,\n",
    "            \"verbose\": 1,\n",
    "        },\n",
    "        \"notes\": \"Standard processing is fine. No need for optimization.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Medium (50k-200k rows)\",\n",
    "        \"params\": {\n",
    "            \"use_memory_optimized\": True,\n",
    "            \"chunk_size\": 1000,\n",
    "            \"correlation_sample_size\": 50000,\n",
    "            \"enable_memory_monitoring\": True,\n",
    "            \"verbose\": 1,\n",
    "        },\n",
    "        \"notes\": \"Default settings work well. Monitor memory if issues arise.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Large (200k-1M rows)\",\n",
    "        \"params\": {\n",
    "            \"use_memory_optimized\": True,\n",
    "            \"chunk_size\": 500,\n",
    "            \"correlation_sample_size\": 30000,\n",
    "            \"enable_memory_monitoring\": True,\n",
    "            \"verbose\": 1,\n",
    "        },\n",
    "        \"notes\": \"Reduce chunk size and correlation sample size. Consider processing subsets separately.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Very Large (>1M rows)\",\n",
    "        \"params\": {\n",
    "            \"use_memory_optimized\": True,\n",
    "            \"chunk_size\": 250,\n",
    "            \"correlation_sample_size\": 10000,\n",
    "            \"enable_memory_monitoring\": True,\n",
    "            \"force_dask\": True,\n",
    "            \"verbose\": 1,\n",
    "        },\n",
    "        \"notes\": \"Consider splitting into multiple files. Use force_dask if available. Process on high-memory machine.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for config in configs:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Configuration: {config['name']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(\"\\nParameters:\")\n",
    "    for param, value in config[\"params\"].items():\n",
    "        print(f\"  {param:30s} = {value}\")\n",
    "    print(f\"\\nNotes: {config['notes']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Integration with Other Modules\n",
    "\n",
    "How to prepare cleaned data for use with other SysmexCBCTools modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data for downstream analysis\n",
    "processor = XNSampleProcessor(\n",
    "    remove_clotintube=True,\n",
    "    remove_multimeasurementsamples=True,\n",
    "    std_threshold=1.0,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "df_clean = processor.process_files(\n",
    "    \"../data/temp/XN_SAMPLE_part1.csv\", save_output=False\n",
    ")\n",
    "\n",
    "print(f\"Cleaned data shape: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Transfer Module (Cross-Analyser Alignment)\n",
    "\n",
    "The transfer module works with both XN_SAMPLE.csv files and raw flow cytometry data. After cleaning:\n",
    "\n",
    "```python\n",
    "# Save cleaned data for transfer module\n",
    "df_clean.to_csv('cleaned_source.csv', index=False)\n",
    "\n",
    "# Then use with transfer module\n",
    "# from sysmexcbctools.transfer import ...\n",
    "# transform_xnsample(\n",
    "#     source_dataset='cleaned_source.csv',\n",
    "#     target_dataset='cleaned_target.csv',\n",
    "#     ...\n",
    "# )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Correction Module (GAM-based Domain Correction)\n",
    "\n",
    "The correction module expects clean, numeric tabular data:\n",
    "\n",
    "```python\n",
    "# Ensure numeric columns for GAM fitting\n",
    "# Select FBC parameters\n",
    "fbc_cols = ['WBC(10^3/uL)', 'RBC(10^6/uL)', 'HGB(g/dL)', 'PLT(10^3/uL)']\n",
    "df_for_gam = df_clean[fbc_cols].dropna()\n",
    "\n",
    "# Add covariates to correct for\n",
    "df_for_gam['sample_age_days'] = ...  # Calculate from dates\n",
    "df_for_gam['day_of_week'] = ...      # Extract from dates\n",
    "\n",
    "# Save for correction module\n",
    "# df_for_gam.to_csv('for_gam_correction.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Dis-AE 2 Module (Domain-Invariant Learning)\n",
    "\n",
    "The Dis-AE 2 module requires numeric features and integer-encoded labels:\n",
    "\n",
    "```python\n",
    "# Select numeric features (FBC parameters + others)\n",
    "feature_cols = ['WBC(10^3/uL)', 'RBC(10^6/uL)', 'HGB(g/dL)', 'HCT(%)',\n",
    "                'PLT(10^3/uL)', 'MCV(fL)', 'MCH(pg)', 'MCHC(g/dL)']\n",
    "X = df_clean[feature_cols].dropna()\n",
    "\n",
    "# Prepare task labels (e.g., binary classification)\n",
    "# y_task = ...\n",
    "\n",
    "# Prepare domain labels (e.g., analyser ID, site, batch)\n",
    "# y_domain = df_clean['Analyzer ID'].map({'analyzer1': 0, 'analyzer2': 1, ...})\n",
    "\n",
    "# Then use with Dis-AE 2\n",
    "# from sysmexcbctools.disae2.disae2 import DisAE\n",
    "# model = DisAE(input_dim=len(feature_cols), ...)\n",
    "# model.fit(X.values, y_task, y_domain)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bloodcounts_sysmex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
