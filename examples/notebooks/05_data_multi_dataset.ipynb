{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Dataset Consolidation with XNSampleProcessor\n",
    "\n",
    "This notebook demonstrates how to use `XNSampleProcessor` to consolidate and clean multiple XN_SAMPLE.csv files from different sources or decryptions.\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "1. **Multiple decryptions**: Combine data from multiple `.116` file decryptions\n",
    "2. **Multi-site studies**: Consolidate data from different hospitals/analysers\n",
    "3. **Longitudinal studies**: Merge data collected at different time points\n",
    "4. **Batch processing**: Process multiple datasets with consistent parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path if needed\n",
    "sys.path.insert(0, str(Path.cwd().parent.parent))\n",
    "\n",
    "from sysmexcbctools.data import XNSampleProcessor\n",
    "\n",
    "# Plotting imports and styling (used in later cells)\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# PDF-compatible fonts\n",
    "matplotlib.rcParams[\"pdf.fonttype\"] = 42\n",
    "matplotlib.rcParams[\"ps.fonttype\"] = 42\n",
    "\n",
    "# Scientific plot style\n",
    "import scienceplots\n",
    "\n",
    "plt.style.use([\"science\", \"nature\"])\n",
    "\n",
    "# Colourblind-friendly palette\n",
    "SEABORN_PALETTE = \"colorblind\"\n",
    "seaborn_colors = sns.color_palette(SEABORN_PALETTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data\n",
    "\n",
    "For this notebook, we'll load a single XN_SAMPLE.csv file and artificially partition it into three parts to simulate multiple decryptions or data sources. This allows us to demonstrate multi-file consolidation functionality without requiring multiple actual source files.\n",
    "\n",
    "### Loading Data with ConfigLoader\n",
    "\n",
    "First, we'll load the data using the same approach as in the basic cleaning notebook - using the YAML config with fallback to manual path specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path to import sysmexcbctools\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Path to your XN_SAMPLE.csv file\n",
    "# Option 1: Use the config loader (if you have the data_paths.yaml configured)\n",
    "try:\n",
    "    from sysmexcbctools.transfer.config.config_loader import ConfigLoader\n",
    "\n",
    "    config_loader = ConfigLoader(\n",
    "        config_file=str(\n",
    "            project_root / \"sysmexcbctools/transfer/config/data_paths.yaml\"\n",
    "        ),\n",
    "        environment=\"production\",\n",
    "    )\n",
    "\n",
    "    # Get the raw data directory for INTERVAL dataset 36\n",
    "    dataset_dir = config_loader.get_dataset_path(category=\"raw\", dataset=\"interval_36\")\n",
    "    data_path = dataset_dir / \"XN_SAMPLE.csv\"\n",
    "\n",
    "    print(f\"✓ Loaded path from config: {data_path}\")\n",
    "\n",
    "except (FileNotFoundError, ValueError, KeyError) as e:\n",
    "    print(f\"⚠ Could not load from config: {e}\")\n",
    "    print(\"  Falling back to manual path specification...\")\n",
    "\n",
    "    # Option 2: Manually specify your data path\n",
    "    # EDIT THIS PATH to point to your XN_SAMPLE.csv file:\n",
    "    data_path = Path(\"/path/to/your/XN_SAMPLE.csv\")\n",
    "\n",
    "    print(f\"\\n  Please edit this cell and set data_path to your file location.\")\n",
    "    print(f\"  Current (placeholder) path: {data_path}\")\n",
    "\n",
    "print(f\"\\nFile exists: {data_path.exists()}\")\n",
    "if data_path.exists():\n",
    "    print(f\"File size: {data_path.stat().st_size / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full dataset\n",
    "print(\"Loading full dataset...\")\n",
    "df_full = pd.read_csv(data_path, encoding=\"ISO-8859-1\", low_memory=False)\n",
    "print(f\"Loaded {len(df_full):,} rows\")\n",
    "\n",
    "# Split into three parts by date (to simulate different decryptions/time periods)\n",
    "# Sort by date first to make a realistic split\n",
    "if \"Date\" in df_full.columns:\n",
    "    df_full = df_full.sort_values(\"Date\")\n",
    "    split_method = \"chronological (by date)\"\n",
    "else:\n",
    "    # If no date column, just split sequentially\n",
    "    split_method = \"sequential\"\n",
    "\n",
    "n = len(df_full)\n",
    "split_1 = n // 3\n",
    "split_2 = 2 * n // 3\n",
    "\n",
    "df_part1 = df_full.iloc[:split_1].copy()\n",
    "df_part2 = df_full.iloc[split_1:split_2].copy()\n",
    "df_part3 = df_full.iloc[split_2:].copy()\n",
    "\n",
    "print(f\"\\nSplit method: {split_method}\")\n",
    "print(f\"Part 1: {len(df_part1):,} rows ({len(df_part1)/n*100:.1f}%)\")\n",
    "print(f\"Part 2: {len(df_part2):,} rows ({len(df_part2)/n*100:.1f}%)\")\n",
    "print(f\"Part 3: {len(df_part3):,} rows ({len(df_part3)/n*100:.1f}%)\")\n",
    "\n",
    "# Save to temporary files\n",
    "temp_dir = Path(\"../data/temp\")\n",
    "temp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "file_list = [\n",
    "    str(temp_dir / \"XN_SAMPLE_part1.csv\"),\n",
    "    str(temp_dir / \"XN_SAMPLE_part2.csv\"),\n",
    "    str(temp_dir / \"XN_SAMPLE_part3.csv\"),\n",
    "]\n",
    "\n",
    "print(f\"\\nSaving temporary files to: {temp_dir}\")\n",
    "df_part1.to_csv(file_list[0], index=False)\n",
    "df_part2.to_csv(file_list[1], index=False)\n",
    "df_part3.to_csv(file_list[2], index=False)\n",
    "\n",
    "print(\"✓ Test data partitions created successfully!\")\n",
    "print(f\"\\nFile list for processing:\")\n",
    "for f in file_list:\n",
    "    print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Consolidating Multiple Files Directly\n",
    "\n",
    "The simplest approach: provide a list of file paths to `process_files()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process multiple CSV files at once\n",
    "# Using the file_list created in the previous cell\n",
    "\n",
    "# Create processor\n",
    "processor = XNSampleProcessor(verbose=1)  # Show progress messages\n",
    "\n",
    "# Process all files together\n",
    "df_consolidated = processor.process_files(\n",
    "    input_files=file_list, dataset_name=\"consolidated\", save_output=False\n",
    ")\n",
    "\n",
    "print(f\"\\nConsolidated dataset shape: {df_consolidated.shape}\")\n",
    "print(f\"Unique samples: {df_consolidated['Sample No.'].nunique()}\")\n",
    "if \"Date\" in df_consolidated.columns:\n",
    "    print(\n",
    "        f\"Date range: {df_consolidated['Date'].min()} to {df_consolidated['Date'].max()}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens during consolidation?\n",
    "\n",
    "1. **Files are concatenated** before processing\n",
    "2. **Duplicate rows** are removed (identical across all columns)\n",
    "3. **Multiple measurements** per sample are handled according to your settings\n",
    "4. **All cleaning steps** are applied to the combined dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tracking Data Sources\n",
    "\n",
    "When combining multiple files, you may want to track which samples came from which source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files manually to add source tracking\n",
    "dfs = []\n",
    "sources = [\"Part_1\", \"Part_2\", \"Part_3\"]\n",
    "\n",
    "for file_path, source in zip(file_list, sources):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"DataSource\"] = source  # Add source identifier\n",
    "    dfs.append(df)\n",
    "    print(f\"Loaded {len(df)} rows from {source}\")\n",
    "\n",
    "# Concatenate\n",
    "df_combined = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"\\nTotal rows before cleaning: {len(df_combined)}\")\n",
    "\n",
    "# Save temporarily\n",
    "temp_file = \"../data/temp_combined.csv\"\n",
    "df_combined.to_csv(temp_file, index=False)\n",
    "\n",
    "# Process with source tracking preserved\n",
    "processor = XNSampleProcessor()\n",
    "df_clean = processor.process_files(temp_file, save_output=False)\n",
    "\n",
    "# Check source distribution\n",
    "print(\"\\nSamples per source after cleaning:\")\n",
    "print(df_clean[\"DataSource\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Config-Based Multi-Dataset Processing\n",
    "\n",
    "For complex projects with many datasets, use a YAML configuration file to define all your data sources and processing parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example config structure\n",
    "config_example = \"\"\"\n",
    "input:\n",
    "  datasets:\n",
    "    - name: INTERVAL_baseline\n",
    "      files:\n",
    "        - /path/to/INTERVAL/XN_SAMPLE_baseline.csv\n",
    "    \n",
    "    - name: INTERVAL_followup\n",
    "      files:\n",
    "        - /path/to/INTERVAL/XN_SAMPLE_followup.csv\n",
    "    \n",
    "    - name: STRIDES\n",
    "      files:\n",
    "        - /path/to/STRIDES/decryption1/XN_SAMPLE.csv\n",
    "        - /path/to/STRIDES/decryption2/XN_SAMPLE.csv\n",
    "        - /path/to/STRIDES/decryption3/XN_SAMPLE.csv\n",
    "\n",
    "processing:\n",
    "  remove_clotintube: true\n",
    "  remove_multimeasurementsamples: true\n",
    "  std_threshold: 1.0\n",
    "  remove_correlated: false\n",
    "\n",
    "output:\n",
    "  directory: ./processed_data\n",
    "  filename_prefix: XN_SAMPLE_clean\n",
    "\"\"\"\n",
    "\n",
    "print(config_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a real config file for this example\n",
    "import yaml\n",
    "\n",
    "# Use the file_list we created earlier\n",
    "config = {\n",
    "    \"input\": {\n",
    "        \"datasets\": [\n",
    "            {\"name\": \"part1\", \"files\": [file_list[0]]},\n",
    "            {\"name\": \"part2\", \"files\": [file_list[1]]},\n",
    "            {\"name\": \"part3\", \"files\": [file_list[2]]},\n",
    "        ]\n",
    "    },\n",
    "    \"processing\": {\n",
    "        \"remove_clotintube\": True,\n",
    "        \"remove_multimeasurementsamples\": True,\n",
    "        \"std_threshold\": 1.0,\n",
    "    },\n",
    "    \"output\": {\"directory\": \"./output\", \"filename_prefix\": \"XN_SAMPLE_processed\"},\n",
    "}\n",
    "\n",
    "config_path = str(temp_dir / \"multi_dataset_config.yaml\")\n",
    "with open(config_path, \"w\") as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "print(f\"Created config file: {config_path}\")\n",
    "print(\"\\nConfig contents:\")\n",
    "print(yaml.dump(config, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process using config file\n",
    "processor = XNSampleProcessor(config_path=config_path)\n",
    "\n",
    "# Process specific dataset\n",
    "df_part1 = processor.process(\"part1\")\n",
    "print(f\"\\nPart 1 shape: {df_part1.shape}\")\n",
    "\n",
    "df_part2 = processor.process(\"part2\")\n",
    "print(f\"Part 2 shape: {df_part2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch Processing Multiple Datasets\n",
    "\n",
    "Process all datasets defined in your config with consistent parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all datasets in config\n",
    "processor = XNSampleProcessor(\n",
    "    config_path=config_path,\n",
    "    log_to_file=True,  # Save logs and diagnostics for each dataset\n",
    ")\n",
    "\n",
    "results = {}\n",
    "for dataset_config in processor.config[\"input\"][\"datasets\"]:\n",
    "    dataset_name = dataset_config[\"name\"]\n",
    "\n",
    "    try:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing: {dataset_name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        df = processor.process(dataset_name)\n",
    "        results[dataset_name] = df\n",
    "\n",
    "        print(\n",
    "            f\"✓ Completed: {df.shape[0]} rows, {df['Sample No.'].nunique()} unique samples\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed: {e}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n\\n{'='*50}\")\n",
    "print(\"BATCH PROCESSING SUMMARY\")\n",
    "print(f\"{'='*50}\")\n",
    "for name, df in results.items():\n",
    "    print(f\"{name:20s}: {df.shape[0]:6d} rows, {df['Sample No.'].nunique():6d} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Handling Overlapping Samples\n",
    "\n",
    "What if the same sample appears in multiple files? The processor handles this automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate sample IDs across files\n",
    "sample_ids_by_file = []\n",
    "\n",
    "for file_path in file_list:\n",
    "    df = pd.read_csv(file_path)\n",
    "    sample_ids = set(df[\"Sample No.\"].unique())\n",
    "    sample_ids_by_file.append(sample_ids)\n",
    "    print(f\"{Path(file_path).name}: {len(sample_ids)} unique samples\")\n",
    "\n",
    "# Find overlaps\n",
    "all_samples = set.union(*sample_ids_by_file)\n",
    "overlapping_samples = set()\n",
    "for i in range(len(sample_ids_by_file)):\n",
    "    for j in range(i + 1, len(sample_ids_by_file)):\n",
    "        overlap = sample_ids_by_file[i] & sample_ids_by_file[j]\n",
    "        overlapping_samples.update(overlap)\n",
    "\n",
    "print(f\"\\nTotal unique samples across all files: {len(all_samples)}\")\n",
    "print(f\"Samples appearing in multiple files: {len(overlapping_samples)}\")\n",
    "\n",
    "# Process and check\n",
    "processor = XNSampleProcessor()\n",
    "df_clean = processor.process_files(file_list, save_output=False)\n",
    "\n",
    "print(f\"\\nAfter processing:\")\n",
    "print(f\"  Total rows: {len(df_clean)}\")\n",
    "print(f\"  Unique samples: {df_clean['Sample No.'].nunique()}\")\n",
    "print(\n",
    "    f\"\\nDuplicate handling: {'remove_duplicate_rows()' if len(df_clean) < sum(len(pd.read_csv(f)) for f in file_list) else 'no duplicates found'}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How duplicate samples are handled:\n",
    "\n",
    "1. **Exact duplicates** (identical across all columns) → removed by `remove_duplicate_rows()`\n",
    "2. **Multiple measurements** (same Sample No., different values) → handled by `handle_multiple_measurements()`\n",
    "   - If measurements agree (within std_threshold) → keep earliest\n",
    "   - If measurements disagree → flagged for manual review (saved to diagnostic file if `log_to_file=True`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Custom Parameters for Different Datasets\n",
    "\n",
    "Sometimes you need different processing parameters for different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process dataset with strict parameters\n",
    "processor_strict = XNSampleProcessor(\n",
    "    remove_clotintube=True,\n",
    "    remove_multimeasurementsamples=True,\n",
    "    std_threshold=0.5,  # More strict - fewer measurements will be considered \"matching\"\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "df_strict = processor_strict.process_files(\n",
    "    file_list[0],  # Using first partition from file_list\n",
    "    dataset_name=\"part1_strict\",\n",
    "    save_output=False,\n",
    ")\n",
    "\n",
    "# Process same dataset with lenient parameters\n",
    "processor_lenient = XNSampleProcessor(\n",
    "    remove_clotintube=False,  # Keep clotted samples\n",
    "    remove_multimeasurementsamples=True,\n",
    "    std_threshold=2.0,  # More lenient - more measurements will be considered \"matching\"\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "df_lenient = processor_lenient.process_files(\n",
    "    file_list[0],  # Using first partition from file_list\n",
    "    dataset_name=\"part1_lenient\",\n",
    "    save_output=False,\n",
    ")\n",
    "\n",
    "print(f\"\\nStrict processing: {len(df_strict)} rows\")\n",
    "print(f\"Lenient processing: {len(df_lenient)} rows\")\n",
    "print(\n",
    "    f\"Difference: {len(df_lenient) - len(df_strict)} rows ({(len(df_lenient) - len(df_strict))/len(df_strict)*100:.1f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparing Datasets After Processing\n",
    "\n",
    "Once you've processed multiple datasets, you may want to compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare FBC parameter distributions\n",
    "# Use datasets from section 4, or create fresh if needed\n",
    "if \"results\" not in locals() or not results:\n",
    "    print(\"Loading datasets for comparison...\")\n",
    "    processor = XNSampleProcessor()\n",
    "    results = {}\n",
    "    for i, part_file in enumerate(file_list[:2], 1):  # Use first two partitions\n",
    "        part_name = f\"XN_SAMPLE_part{i}\"\n",
    "        results[part_name] = processor.process_files(part_file, save_output=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(6.6, 4.5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "fbc_params = [\n",
    "    r\"WBC(10$^3$/uL)\",\n",
    "    r\"RBC(10$^6$/uL)\",\n",
    "    r\"HGB(g/dL)\",\n",
    "    r\"PLT(10$^3$/uL)\",\n",
    "    r\"MCV(fL)\",\n",
    "    r\"MCH(pg)\",\n",
    "]\n",
    "\n",
    "# Map display names to actual column names in the dataframe\n",
    "param_column_map = {\n",
    "    r\"WBC(10$^3$/uL)\": \"WBC(10^3/uL)\",\n",
    "    r\"RBC(10$^6$/uL)\": \"RBC(10^6/uL)\",\n",
    "    r\"HGB(g/dL)\": \"HGB(g/dL)\",\n",
    "    r\"PLT(10$^3$/uL)\": \"PLT(10^3/uL)\",\n",
    "    r\"MCV(fL)\": \"MCV(fL)\",\n",
    "    r\"MCH(pg)\": \"MCH(pg)\",\n",
    "}\n",
    "\n",
    "for i, param_display in enumerate(fbc_params):\n",
    "    ax = axes[i]\n",
    "    param_column = param_column_map[param_display]\n",
    "\n",
    "    # Check if parameter exists in any dataset\n",
    "    plotted_any = False\n",
    "    for name, df in results.items():\n",
    "        if param_column in df.columns:\n",
    "            data = df[param_column].dropna()\n",
    "            if len(data) > 0:\n",
    "                ax.hist(\n",
    "                    data,\n",
    "                    alpha=0.5,\n",
    "                    label=name,\n",
    "                    bins=50,\n",
    "                    edgecolor=\"black\",\n",
    "                    linewidth=0.5,\n",
    "                )\n",
    "                plotted_any = True\n",
    "\n",
    "    if plotted_any:\n",
    "        ax.set_xlabel(param_display)\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "        ax.legend()\n",
    "        ax.set_title(f\"{param_display} Distribution\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax.text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            f\"{param_display}\\nNo data available\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            transform=ax.transAxes,\n",
    "        )\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Merging Processed Datasets\n",
    "\n",
    "After processing datasets separately, you might want to combine them for joint analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process datasets separately first\n",
    "processor = XNSampleProcessor()\n",
    "\n",
    "df1 = processor.process_files(\n",
    "    file_list[0], dataset_name=\"part1\", save_output=False  # First partition\n",
    ")\n",
    "df1[\"data_source\"] = \"Part 1\"\n",
    "\n",
    "df2 = processor.process_files(\n",
    "    file_list[1], dataset_name=\"part2\", save_output=False  # Second partition\n",
    ")\n",
    "df2[\"data_source\"] = \"Part 2\"\n",
    "\n",
    "# Merge\n",
    "df_merged = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "print(f\"\\nMerged dataset:\")\n",
    "print(f\"  Total rows: {len(df_merged)}\")\n",
    "print(f\"  Unique samples: {df_merged['Sample No.'].nunique()}\")\n",
    "print(f\"\\nSamples by source:\")\n",
    "print(df_merged[\"data_source\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Saving Processed Datasets\n",
    "\n",
    "Options for saving your processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Save automatically during processing\n",
    "processor = XNSampleProcessor(\n",
    "    output_dir=\"./processed_data\",\n",
    "    output_prefix=\"study_clean\",\n",
    "    log_to_file=True,  # Also save logs and diagnostics\n",
    ")\n",
    "\n",
    "df = processor.process_files(\n",
    "    file_list, dataset_name=\"my_study\", save_output=True  # Automatically saves\n",
    ")\n",
    "\n",
    "print(f\"\\nSaved to: {processor.output_dir}/study_clean_my_study_YYYYMMDD_HHMMSS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Save manually after processing\n",
    "processor = XNSampleProcessor()\n",
    "df = processor.process_files(file_list, save_output=False)\n",
    "\n",
    "# Save with custom name\n",
    "output_path = \"./my_custom_output.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Saved to: {output_path}\")\n",
    "\n",
    "# Or save in other formats\n",
    "df.to_parquet(\"./my_output.parquet\", index=False)\n",
    "df.to_pickle(\"./my_output.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Points for Multi-Dataset Processing:\n",
    "\n",
    "1. **Direct file lists**: Simple and flexible for quick processing\n",
    "2. **Config files**: Best for complex projects with many datasets\n",
    "3. **Source tracking**: Add identifier columns before processing\n",
    "4. **Duplicate handling**: Automatic removal and flagging\n",
    "5. **Batch processing**: Process multiple datasets consistently\n",
    "6. **Custom parameters**: Tailor processing to each dataset's needs\n",
    "7. **Merging**: Combine processed datasets for joint analysis\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "- Always check for overlapping samples across datasets\n",
    "- Use config files for reproducibility\n",
    "- Enable `log_to_file=True` for important processing runs\n",
    "- Review diagnostic files for samples with discrepant measurements\n",
    "- Save intermediate results to avoid reprocessing\n",
    "- Document data sources and processing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup temporary files (uncomment to execute)\n",
    "# import shutil\n",
    "# if temp_dir.exists():\n",
    "#     shutil.rmtree(temp_dir)\n",
    "#     print(f\"✓ Removed temporary directory: {temp_dir}\")\n",
    "# else:\n",
    "#     print(\"No temporary files to clean up\")\n",
    "\n",
    "print(\"(Uncomment the code above to remove temporary files)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bloodcounts_sysmex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
