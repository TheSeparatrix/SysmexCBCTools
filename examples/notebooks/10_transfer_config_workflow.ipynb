{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration-Based Transformation Workflow\n",
    "\n",
    "This notebook demonstrates how to use the configuration system for end-to-end analyser alignment workflows.\n",
    "In the previous notebooks, we have used the config already to load the paths to the files. We will now see that the API supports just inputting the config directly.\n",
    "\n",
    "**Why use configuration-based workflows?**\n",
    "- **Reproducibility**: All paths and parameters centralized in config files\n",
    "- **Portability**: Easy to switch between development/production environments\n",
    "- **Maintainability**: Update paths once, apply everywhere\n",
    "- **Collaboration**: Team members use same logical dataset names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# PDF-compatible fonts\n",
    "matplotlib.rcParams[\"pdf.fonttype\"] = 42\n",
    "matplotlib.rcParams[\"ps.fonttype\"] = 42\n",
    "\n",
    "# Scientific plot style\n",
    "import scienceplots\n",
    "\n",
    "plt.style.use([\"science\", \"nature\"])\n",
    "\n",
    "# Colourblind-friendly palette\n",
    "SEABORN_PALETTE = \"colorblind\"\n",
    "seaborn_colors = sns.color_palette(SEABORN_PALETTE)\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, str(Path.cwd().parent.parent))\n",
    "\n",
    "from sysmexcbctools.transfer.sysmexalign import (\n",
    "    FlowTransformer,\n",
    "    ImpedanceTransformer,\n",
    "    XNSampleTransformer,\n",
    ")\n",
    "from sysmexcbctools.transfer.config import ConfigLoader, get_config_loader\n",
    "from sysmexcbctools.transfer.utils import DataLoader, load_centile_samples\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Configuration System\n",
    "\n",
    "The configuration system uses `config/data_paths.yaml` to map logical dataset names to actual file paths. (Again, our specific files are used. Users will have to create their own configs for their datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = \"../../sysmexcbctools/transfer/config/data_paths.yaml\"\n",
    "config = get_config_loader(config_path)\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"Config file: {config_path}\")\n",
    "print(f\"\\nAvailable datasets:\")\n",
    "\n",
    "# List available datasets using the public API\n",
    "datasets = config.list_datasets()\n",
    "for category in [\"raw\", \"processed\"]:\n",
    "    print(f\"\\n{category.upper()}:\")\n",
    "    if category in datasets:\n",
    "        for dataset_name in datasets[category].keys():\n",
    "            path = config.get_dataset_path(category, dataset_name)\n",
    "            exists = os.path.exists(path)\n",
    "            status = \"✓\" if exists else \"✗\"\n",
    "            print(f\"  {status} {dataset_name}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Sample Numbers Using Config\n",
    "\n",
    "Instead of hardcoding paths to sample number files, we use logical names from the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample numbers using config\n",
    "print(\"Loading sample numbers from config...\\n\")\n",
    "\n",
    "# Source: INTERVAL baseline analyser 36\n",
    "source_samples = load_centile_samples(\"interval_baseline_36\", config_path)\n",
    "print(f\"Source samples (interval_baseline_36): {len(source_samples):,} samples\")\n",
    "\n",
    "# Target: INTERVAL baseline analyser 41\n",
    "target_samples = load_centile_samples(\"interval_baseline_41\", config_path)\n",
    "print(f\"Target samples (interval_baseline_41): {len(target_samples):,} samples\")\n",
    "\n",
    "print(f\"\\nSample number format: {source_samples[0]} (type: {type(source_samples[0])})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Flow Cytometry Transformation Using Config\n",
    "\n",
    "Let's transform RET (reticulocyte) flow cytometry data using the config-based approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define datasets using logical names\n",
    "source_dataset_name = \"interval_36\"\n",
    "target_dataset_name = \"interval_41\"\n",
    "channel = \"RET\"\n",
    "\n",
    "# Get actual paths from config\n",
    "source_path = config.get_dataset_path(\"raw\", source_dataset_name)\n",
    "target_path = config.get_dataset_path(\"raw\", target_dataset_name)\n",
    "\n",
    "print(f\"Source dataset: {source_dataset_name}\")\n",
    "print(f\"  Path: {source_path}\")\n",
    "print(f\"\\nTarget dataset: {target_dataset_name}\")\n",
    "print(f\"  Path: {target_path}\")\n",
    "print(f\"\\nChannel: {channel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit FlowTransformer\n",
    "print(f\"\\nCreating FlowTransformer for {channel} channel...\\n\")\n",
    "\n",
    "transformer = FlowTransformer(\n",
    "    channel=channel,\n",
    "    n_components=20,\n",
    "    max_samples=50000,\n",
    "    save_fitted_data=True,  # Save data for visualization\n",
    ")\n",
    "\n",
    "# Get file lists\n",
    "source_files = sorted(glob(f\"{source_path}/SCT/{channel}_*.116.csv\"))\n",
    "target_files = sorted(glob(f\"{target_path}/SCT/{channel}_*.116.csv\"))\n",
    "\n",
    "print(f\"Source files: {len(source_files)}\")\n",
    "print(f\"Target files: {len(target_files)}\")\n",
    "\n",
    "# Fit transformer\n",
    "print(f\"\\nFitting transformer...\")\n",
    "transformer.fit(\n",
    "    source_files=source_files,\n",
    "    target_files=target_files,\n",
    "    source_sample_nos=source_samples,\n",
    "    target_sample_nos=target_samples,\n",
    ")\n",
    "\n",
    "print(\"\\nTransformer fitted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save transformer for reuse\n",
    "output_dir = \"../outputs/transformers\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "transformer_path = (\n",
    "    f\"{output_dir}/flow_{channel}_{source_dataset_name}_to_{target_dataset_name}.pkl\"\n",
    ")\n",
    "transformer.save(transformer_path)\n",
    "\n",
    "print(f\"Transformer saved to: {transformer_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch Processing Multiple Flow Cytometry Channels\n",
    "\n",
    "Process all flow cytometry channels (RET, WDF, WNR, PLTF) in a single workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define channels to process\n",
    "channels = [\"RET\", \"WDF\", \"WNR\", \"PLTF\"]\n",
    "\n",
    "# Check which channels have data\n",
    "available_channels = []\n",
    "for ch in channels:\n",
    "    source_ch_files = glob(f\"{source_path}/SCT/{ch}_*.116.csv\")\n",
    "    target_ch_files = glob(f\"{target_path}/SCT/{ch}_*.116.csv\")\n",
    "    if len(source_ch_files) > 0 and len(target_ch_files) > 0:\n",
    "        available_channels.append(ch)\n",
    "        print(\n",
    "            f\"✓ {ch}: {len(source_ch_files)} source files, {len(target_ch_files)} target files\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"✗ {ch}: No data available\")\n",
    "\n",
    "print(f\"\\nWill process {len(available_channels)} channels: {available_channels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch fit transformers for all channels\n",
    "transformers = {}\n",
    "\n",
    "for channel in tqdm(available_channels, desc=\"Fitting transformers\"):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing channel: {channel}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Create transformer\n",
    "    trans = FlowTransformer(\n",
    "        channel=channel,\n",
    "        n_components=20,\n",
    "        max_samples=50000,\n",
    "        save_fitted_data=False,  # Don't save data for batch processing\n",
    "    )\n",
    "\n",
    "    # Get file lists\n",
    "    source_files = sorted(glob(f\"{source_path}/SCT/{channel}_*.116.csv\"))\n",
    "    target_files = sorted(glob(f\"{target_path}/SCT/{channel}_*.116.csv\"))\n",
    "\n",
    "    # Fit\n",
    "    trans.fit(\n",
    "        source_files=source_files,\n",
    "        target_files=target_files,\n",
    "        source_sample_nos=source_samples,\n",
    "        target_sample_nos=target_samples,\n",
    "    )\n",
    "\n",
    "    # Save\n",
    "    save_path = f\"{output_dir}/flow_{channel}_{source_dataset_name}_to_{target_dataset_name}.pkl\"\n",
    "    trans.save(save_path)\n",
    "\n",
    "    transformers[channel] = trans\n",
    "    print(f\"✓ {channel} transformer saved\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Batch processing complete! Fitted {len(transformers)} transformers.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Impedance Transformation Using Config\n",
    "\n",
    "Now let's transform impedance data (RBC and PLT histograms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OutputData.csv files\n",
    "print(\"Loading impedance data (OutputData.csv)...\\n\")\n",
    "\n",
    "source_impedance_path = f\"{source_path}/OutputData.csv\"\n",
    "target_impedance_path = f\"{target_path}/OutputData.csv\"\n",
    "\n",
    "print(f\"Source: {source_impedance_path}\")\n",
    "print(f\"Target: {target_impedance_path}\")\n",
    "\n",
    "# Check files exist\n",
    "if os.path.exists(source_impedance_path) and os.path.exists(target_impedance_path):\n",
    "    print(\"\\n✓ Both impedance files found\")\n",
    "\n",
    "    # Load data\n",
    "    source_impedance_df = pd.read_csv(source_impedance_path, low_memory=False)\n",
    "    target_impedance_df = pd.read_csv(target_impedance_path, low_memory=False)\n",
    "\n",
    "    print(f\"Source shape: {source_impedance_df.shape}\")\n",
    "    print(f\"Target shape: {target_impedance_df.shape}\")\n",
    "else:\n",
    "    print(\"\\n✗ Impedance files not found - skipping impedance transformation\")\n",
    "    source_impedance_df = None\n",
    "    target_impedance_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit ImpedanceTransformer\n",
    "if source_impedance_df is not None:\n",
    "    print(\"\\nCreating ImpedanceTransformer...\\n\")\n",
    "\n",
    "    impedance_transformer = ImpedanceTransformer(\n",
    "        # n_components=20,\n",
    "        gmm_sample_size=50000,\n",
    "        # max_samples=100000\n",
    "    )\n",
    "\n",
    "    # Fit transformer\n",
    "    impedance_transformer.fit(\n",
    "        source_df=source_impedance_df,\n",
    "        target_df=target_impedance_df,\n",
    "        source_sample_nos=source_samples,\n",
    "        target_sample_nos=target_samples,\n",
    "    )\n",
    "\n",
    "    # Save transformer\n",
    "    impedance_path = (\n",
    "        f\"{output_dir}/impedance_{source_dataset_name}_to_{target_dataset_name}.pkl\"\n",
    "    )\n",
    "    impedance_transformer.save(impedance_path)\n",
    "\n",
    "    print(f\"\\nImpedance transformer saved to: {impedance_path}\")\n",
    "else:\n",
    "    print(\"\\nSkipping impedance transformation (no data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. XN_SAMPLE Transformation Using Config\n",
    "\n",
    "Finally, let's transform the tabular XN_SAMPLE data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load XN_SAMPLE files\n",
    "print(\"Loading XN_SAMPLE data...\\n\")\n",
    "\n",
    "source_xnsample_path = f\"{source_path}/XN_SAMPLE.csv\"\n",
    "target_xnsample_path = f\"{target_path}/XN_SAMPLE.csv\"\n",
    "\n",
    "print(f\"Source: {source_xnsample_path}\")\n",
    "print(f\"Target: {target_xnsample_path}\")\n",
    "\n",
    "# Check files exist\n",
    "if os.path.exists(source_xnsample_path) and os.path.exists(target_xnsample_path):\n",
    "    print(\"\\n✓ Both XN_SAMPLE files found\")\n",
    "\n",
    "    # Load data\n",
    "    source_xnsample_df = pd.read_csv(source_xnsample_path, low_memory=False)\n",
    "    target_xnsample_df = pd.read_csv(target_xnsample_path, low_memory=False)\n",
    "\n",
    "    print(f\"Source shape: {source_xnsample_df.shape}\")\n",
    "    print(f\"Target shape: {target_xnsample_df.shape}\")\n",
    "else:\n",
    "    print(\"\\n✗ XN_SAMPLE files not found - skipping XN_SAMPLE transformation\")\n",
    "    source_xnsample_df = None\n",
    "    target_xnsample_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns to transform\n",
    "if source_xnsample_df is not None:\n",
    "    fbc_columns = [\n",
    "        \"RBC(10^6/uL)\",\n",
    "        \"HGB(g/dL)\",\n",
    "        \"HCT(%)\",\n",
    "        \"MCV(fL)\",\n",
    "        \"MCH(pg)\",\n",
    "        \"MCHC(g/dL)\",\n",
    "        \"RDW-SD(fL)\",\n",
    "        \"RDW-CV(%)\",\n",
    "        \"WBC(10^3/uL)\",\n",
    "        \"NEUT#(10^3/uL)\",\n",
    "        \"LYMPH#(10^3/uL)\",\n",
    "        \"MONO#(10^3/uL)\",\n",
    "        \"EO#(10^3/uL)\",\n",
    "        \"BASO#(10^3/uL)\",\n",
    "        \"NEUT%(%)\",\n",
    "        \"LYMPH%(%)\",\n",
    "        \"MONO%(%)\",\n",
    "        \"EO%(%)\",\n",
    "        \"BASO%(%)\",\n",
    "        \"PLT(10^3/uL)\",\n",
    "        \"MPV(fL)\",\n",
    "        \"PCT(%)\",\n",
    "        \"PDW(fL)\",\n",
    "    ]\n",
    "\n",
    "    # Filter to available columns\n",
    "    source_cols = set(source_xnsample_df.columns)\n",
    "    target_cols = set(target_xnsample_df.columns)\n",
    "    columns_to_transform = [\n",
    "        col for col in fbc_columns if col in source_cols and col in target_cols\n",
    "    ]\n",
    "\n",
    "    print(f\"Columns to transform: {len(columns_to_transform)} / {len(fbc_columns)}\")\n",
    "    print(f\"Available columns: {sorted(columns_to_transform)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit XNSampleTransformer\n",
    "if source_xnsample_df is not None:\n",
    "    print(\"\\nCreating XNSampleTransformer...\\n\")\n",
    "\n",
    "    xnsample_transformer = XNSampleTransformer(columns=columns_to_transform)\n",
    "\n",
    "    # Fit transformer\n",
    "    xnsample_transformer.fit(\n",
    "        source_df=source_xnsample_df,\n",
    "        target_df=target_xnsample_df,\n",
    "        source_sample_nos=source_samples,\n",
    "        target_sample_nos=target_samples,\n",
    "    )\n",
    "\n",
    "    # Save transformer\n",
    "    xnsample_path = (\n",
    "        f\"{output_dir}/xnsample_{source_dataset_name}_to_{target_dataset_name}.pkl\"\n",
    "    )\n",
    "    xnsample_transformer.save(xnsample_path)\n",
    "\n",
    "    print(f\"\\nXN_SAMPLE transformer saved to: {xnsample_path}\")\n",
    "else:\n",
    "    print(\"\\nSkipping XN_SAMPLE transformation (no data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary of Created Transformers\n",
    "\n",
    "Let's summarize all the transformation models we've created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all saved transformers\n",
    "print(\"Saved Transformation Models:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "transformer_files = sorted(glob(f\"{output_dir}/*.pkl\"))\n",
    "\n",
    "for tf_file in transformer_files:\n",
    "    filename = os.path.basename(tf_file)\n",
    "    size_mb = os.path.getsize(tf_file) / (1024 * 1024)\n",
    "    print(f\"  {filename:<60} ({size_mb:>6.2f} MB)\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total transformers: {len(transformer_files)}\")\n",
    "total_size = sum([os.path.getsize(f) for f in transformer_files]) / (1024 * 1024)\n",
    "print(f\"Total size: {total_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Loading and Reusing Transformers\n",
    "\n",
    "Once transformers are saved, you can reload them without refitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load a saved transformer\n",
    "print(\"Testing transformer loading...\\n\")\n",
    "\n",
    "# Load flow transformer\n",
    "if len(available_channels) > 0:\n",
    "    test_channel = available_channels[0]\n",
    "    test_path = f\"{output_dir}/flow_{test_channel}_{source_dataset_name}_to_{target_dataset_name}.pkl\"\n",
    "\n",
    "    print(f\"Loading {test_channel} transformer from: {test_path}\")\n",
    "    loaded_flow = FlowTransformer.load(test_path)\n",
    "    print(f\"✓ Loaded successfully! Channel: {loaded_flow.channel}\")\n",
    "\n",
    "# Load impedance transformer\n",
    "if source_impedance_df is not None:\n",
    "    test_path = (\n",
    "        f\"{output_dir}/impedance_{source_dataset_name}_to_{target_dataset_name}.pkl\"\n",
    "    )\n",
    "    print(f\"\\nLoading impedance transformer from: {test_path}\")\n",
    "    loaded_impedance = ImpedanceTransformer.load(test_path)\n",
    "    print(f\"✓ Loaded successfully! GMMs fitted: {loaded_impedance.is_fitted_}\")\n",
    "\n",
    "# Load XN_SAMPLE transformer\n",
    "if source_xnsample_df is not None:\n",
    "    test_path = (\n",
    "        f\"{output_dir}/xnsample_{source_dataset_name}_to_{target_dataset_name}.pkl\"\n",
    "    )\n",
    "    print(f\"\\nLoading XN_SAMPLE transformer from: {test_path}\")\n",
    "    loaded_xnsample = XNSampleTransformer.load(test_path)\n",
    "    print(f\"✓ Loaded successfully! Columns: {len(loaded_xnsample.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Applying Transformations to New Data\n",
    "\n",
    "Once transformers are fitted and saved, you can apply them to new data from the same source analyser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Transform a subset of XN_SAMPLE data\n",
    "if source_xnsample_df is not None:\n",
    "    print(\"Example: Transforming a subset of XN_SAMPLE data\\n\")\n",
    "\n",
    "    # Take first 1000 samples\n",
    "    test_subset = source_xnsample_df.head(1000).copy()\n",
    "    print(f\"Test subset: {test_subset.shape}\")\n",
    "\n",
    "    # Transform\n",
    "    transformed_subset = loaded_xnsample.transform(test_subset)\n",
    "    print(f\"Transformed subset: {transformed_subset.shape}\")\n",
    "\n",
    "    # Show example transformation for one column\n",
    "    example_col = columns_to_transform[0]\n",
    "    print(f\"\\nExample transformation for {example_col}:\")\n",
    "\n",
    "    original_vals = pd.to_numeric(test_subset[example_col], errors=\"coerce\").dropna()\n",
    "    transformed_vals = pd.to_numeric(\n",
    "        transformed_subset[example_col], errors=\"coerce\"\n",
    "    ).dropna()\n",
    "\n",
    "    print(\n",
    "        f\"  Original:    mean={original_vals.mean():.4f}, std={original_vals.std():.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Transformed: mean={transformed_vals.mean():.4f}, std={transformed_vals.std():.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Production Workflow Recommendations\n",
    "\n",
    "**Best practices for production pipelines:**\n",
    "\n",
    "### Configuration Management\n",
    "1. **Use version control** for `data_paths.yaml`\n",
    "2. **Separate configs** for development/staging/production\n",
    "3. **Document dataset names** and their purposes\n",
    "\n",
    "### Transformer Management\n",
    "1. **Save all transformers** with descriptive names including dataset names and dates\n",
    "2. **Version transformers** when refitting (e.g., `flow_RET_36_to_41_v2.pkl`)\n",
    "3. **Document transformation parameters** (n_components, max_samples, etc.)\n",
    "\n",
    "### Data Processing\n",
    "1. **Batch process** multiple channels/files to save time\n",
    "2. **Use sample numbers** to select representative subsets for fitting\n",
    "3. **Validate transformations** before using in production\n",
    "4. **Log all operations** including file paths, parameters, and timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Example: Automated Batch Pipeline\n",
    "\n",
    "Here's a complete example of an automated pipeline that could be used in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transformation_pipeline(\n",
    "    config_path,\n",
    "    source_dataset,\n",
    "    target_dataset,\n",
    "    source_samples_name,\n",
    "    target_samples_name,\n",
    "    output_dir,\n",
    "    flow_channels=[\"RET\", \"WDF\", \"WNR\", \"PLTF\"],\n",
    "    process_impedance=True,\n",
    "    process_xnsample=True,\n",
    "    force_refit=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete automated transformation pipeline.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config_path : str\n",
    "        Path to data_paths.yaml\n",
    "    source_dataset : str\n",
    "        Source dataset name (e.g., 'interval_36')\n",
    "    target_dataset : str\n",
    "        Target dataset name (e.g., 'interval_41')\n",
    "    source_samples_name : str\n",
    "        Name of source sample number file (e.g., 'interval_baseline_36')\n",
    "    target_samples_name : str\n",
    "        Name of target sample number file (e.g., 'interval_baseline_41')\n",
    "    output_dir : str\n",
    "        Directory to save transformers\n",
    "    flow_channels : list\n",
    "        List of flow cytometry channels to process\n",
    "    process_impedance : bool\n",
    "        Whether to process impedance data\n",
    "    process_xnsample : bool\n",
    "        Whether to process XN_SAMPLE data\n",
    "    force_refit : bool\n",
    "        If True, refit even if transformer exists\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary of transformer paths\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"AUTOMATED TRANSFORMATION PIPELINE\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Load config\n",
    "    config = get_config_loader(config_path)\n",
    "    print(f\"\\n✓ Config loaded: {config_path}\")\n",
    "\n",
    "    # Get paths\n",
    "    source_path = config.get_dataset_path(\"raw\", source_dataset)\n",
    "    target_path = config.get_dataset_path(\"raw\", target_dataset)\n",
    "    print(f\"✓ Source path: {source_path}\")\n",
    "    print(f\"✓ Target path: {target_path}\")\n",
    "\n",
    "    # Load samples - pass config_path string, not config object\n",
    "    source_samples = load_centile_samples(source_samples_name, config_path)\n",
    "    target_samples = load_centile_samples(target_samples_name, config_path)\n",
    "    print(f\"✓ Source samples: {len(source_samples):,}\")\n",
    "    print(f\"✓ Target samples: {len(target_samples):,}\")\n",
    "\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"✓ Output directory: {output_dir}\")\n",
    "\n",
    "    transformer_paths = {}\n",
    "\n",
    "    # Process flow cytometry channels\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"FLOW CYTOMETRY CHANNELS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for channel in flow_channels:\n",
    "        save_path = (\n",
    "            f\"{output_dir}/flow_{channel}_{source_dataset}_to_{target_dataset}.pkl\"\n",
    "        )\n",
    "\n",
    "        if os.path.exists(save_path) and not force_refit:\n",
    "            print(f\"\\n✓ {channel}: Transformer already exists (skipping)\")\n",
    "            transformer_paths[f\"flow_{channel}\"] = save_path\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n→ Processing {channel}...\")\n",
    "\n",
    "        # Get files\n",
    "        source_files = sorted(glob(f\"{source_path}/SCT/{channel}_*.116.csv\"))\n",
    "        target_files = sorted(glob(f\"{target_path}/SCT/{channel}_*.116.csv\"))\n",
    "\n",
    "        if len(source_files) == 0 or len(target_files) == 0:\n",
    "            print(f\"  ✗ No data found - skipping\")\n",
    "            continue\n",
    "\n",
    "        print(f\"  Files: {len(source_files)} source, {len(target_files)} target\")\n",
    "\n",
    "        # Fit transformer\n",
    "        trans = FlowTransformer(channel=channel, n_components=20, max_samples=50000)\n",
    "        trans.fit(source_files, target_files, source_samples, target_samples)\n",
    "        trans.save(save_path)\n",
    "\n",
    "        transformer_paths[f\"flow_{channel}\"] = save_path\n",
    "        print(f\"  ✓ Saved: {save_path}\")\n",
    "\n",
    "    # Process impedance\n",
    "    if process_impedance:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"IMPEDANCE DATA\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        save_path = f\"{output_dir}/impedance_{source_dataset}_to_{target_dataset}.pkl\"\n",
    "\n",
    "        if os.path.exists(save_path) and not force_refit:\n",
    "            print(f\"\\n✓ Impedance transformer already exists (skipping)\")\n",
    "            transformer_paths[\"impedance\"] = save_path\n",
    "        else:\n",
    "            source_imp = f\"{source_path}/OutputData.csv\"\n",
    "            target_imp = f\"{target_path}/OutputData.csv\"\n",
    "\n",
    "            if os.path.exists(source_imp) and os.path.exists(target_imp):\n",
    "                print(f\"\\n→ Processing impedance data...\")\n",
    "\n",
    "                source_df = pd.read_csv(source_imp, low_memory=False)\n",
    "                target_df = pd.read_csv(target_imp, low_memory=False)\n",
    "\n",
    "                trans = ImpedanceTransformer(n_components=20, gmm_sample_size=50000)\n",
    "                trans.fit(source_df, target_df, source_samples, target_samples)\n",
    "                trans.save(save_path)\n",
    "\n",
    "                transformer_paths[\"impedance\"] = save_path\n",
    "                print(f\"  ✓ Saved: {save_path}\")\n",
    "            else:\n",
    "                print(f\"\\n✗ Impedance data not found - skipping\")\n",
    "\n",
    "    # Process XN_SAMPLE\n",
    "    if process_xnsample:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"XN_SAMPLE DATA\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        save_path = f\"{output_dir}/xnsample_{source_dataset}_to_{target_dataset}.pkl\"\n",
    "\n",
    "        if os.path.exists(save_path) and not force_refit:\n",
    "            print(f\"\\n✓ XN_SAMPLE transformer already exists (skipping)\")\n",
    "            transformer_paths[\"xnsample\"] = save_path\n",
    "        else:\n",
    "            source_xn = f\"{source_path}/XN_SAMPLE.csv\"\n",
    "            target_xn = f\"{target_path}/XN_SAMPLE.csv\"\n",
    "\n",
    "            if os.path.exists(source_xn) and os.path.exists(target_xn):\n",
    "                print(f\"\\n→ Processing XN_SAMPLE data...\")\n",
    "\n",
    "                source_df = pd.read_csv(source_xn, low_memory=False)\n",
    "                target_df = pd.read_csv(target_xn, low_memory=False)\n",
    "\n",
    "                # Define columns\n",
    "                fbc_cols = [\n",
    "                    \"RBC(10^6/uL)\",\n",
    "                    \"HGB(g/dL)\",\n",
    "                    \"HCT(%)\",\n",
    "                    \"MCV(fL)\",\n",
    "                    \"MCH(pg)\",\n",
    "                    \"MCHC(g/dL)\",\n",
    "                    \"WBC(10^3/uL)\",\n",
    "                    \"PLT(10^3/uL)\",\n",
    "                    \"MPV(fL)\",\n",
    "                ]\n",
    "                available_cols = [\n",
    "                    c\n",
    "                    for c in fbc_cols\n",
    "                    if c in source_df.columns and c in target_df.columns\n",
    "                ]\n",
    "\n",
    "                trans = XNSampleTransformer(columns=available_cols)\n",
    "                trans.fit(source_df, target_df, source_samples, target_samples)\n",
    "                trans.save(save_path)\n",
    "\n",
    "                transformer_paths[\"xnsample\"] = save_path\n",
    "                print(f\"  ✓ Saved: {save_path}\")\n",
    "            else:\n",
    "                print(f\"\\n✗ XN_SAMPLE data not found - skipping\")\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PIPELINE COMPLETE\")\n",
    "    print(f\"Created {len(transformer_paths)} transformers\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    return transformer_paths\n",
    "\n",
    "\n",
    "print(\"Pipeline function defined! Ready to use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of automated pipeline\n",
    "print(\"Example: Running automated pipeline\\n\")\n",
    "\n",
    "results = create_transformation_pipeline(\n",
    "    config_path=config_path,\n",
    "    source_dataset=\"interval_36\",\n",
    "    target_dataset=\"interval_41\",\n",
    "    source_samples_name=\"interval_baseline_36\",\n",
    "    target_samples_name=\"interval_baseline_41\",\n",
    "    output_dir=\"../outputs/transformers\",\n",
    "    flow_channels=[\"RET\"],  # Just RET for demo\n",
    "    process_impedance=True,\n",
    "    process_xnsample=True,\n",
    "    force_refit=False,\n",
    ")\n",
    "\n",
    "print(\"\\nCreated transformers:\")\n",
    "for name, path in results.items():\n",
    "    print(f\"  {name}: {path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bloodcounts_sysmex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
